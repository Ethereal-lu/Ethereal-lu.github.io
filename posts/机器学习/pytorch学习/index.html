<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>PyTorch笔记 | lu</title>
<meta name=keywords content><meta name=description content="torch的tensor默认为FloatTensor类型，可以使用torch.set_default_tensor_type(torch.DoubleTensor)设置默认类型为DoubleTensor，pytorch和numpy的张量在内存中都是默认按行优先顺序排列。

基础
生成张量
x = torch.empty(5, 3) == torch.Tensor(5, 3) # 构造一个未初始化的5✖3的矩阵，里面都是任意的随机数
y = torch.rand(5, 3)   # 构造一个随机初始化的5✖3的矩阵，里面是从区间(0, 1)的均匀分布中抽取出的一组随机数
z = torch.randn(5, 3)   # 构造一个随机初始化的5✖3的矩阵，里面是从标准正态分布中抽取出的一组随机数
a = torch.zeros(5, 3, dtype=torch.int)  # 0填充，类型为整数，还可以是别的     torch.ones(5, 3)  1填充  torch.eye(3) 3×3的单位矩阵
b = torch.tensor([1, 2, 3, 4])  # 用数据直接构建向量
c = torch.randn_like(b, dtype=torch.float)  # 构造形状与b相同的随机数组,torch.ones_like,torch.zeros_like,形状相同的全1.全0

torch.randint(1, 10, [5, 3])  # 构造一个5✖3的矩阵，里面是从[1, 10)中抽取出的一组随机整数
torch.normal(mean, std)   # mean和std都是tensor，随机从mean为均值，std为方差的正太分布取值。输出形状为mean的形状。每一项一一对应，相互独立
torch.full([5, 3], 6)   # 生成一个5✖3的矩阵，并将元素全部填充为6
torch.arange(0, 10)     # tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
torch.arange(0, 10, 2)  # tensor([0, 2, 4, 6, 8])
torch.linspace(0, 10, steps=4)  # tensor([ 0.0000,  3.3333,  6.6667, 10.0000])
torch.logspace(0, 1, steps=10)  # 对数函数中，y轴上在[0, 1]均匀取10个数，返回其对应的x轴上的值

常用函数及操作
idx = torch.randperm(4)   a[idx]    # idx == tensor([2, 0, 3, 1])   打乱顺序。将a的第一维度按照idx的顺序重新排列，idx的顺序随机生成

a[i, j] 获取张量a中位于(i, j) 处的元素，类似于python中的a[i][j]

a.dim() 返回张量的维度
c.shape == c.size()  都是返回数组形状，返回的是元组
list(c.shape)   将结果变为python中的list
c.size(i)   返回上方所得元组中的第i个元素
c.numel()   返回各维度的乘积

x + y == torch.add(x, y)   两数组相加
乘法： 二维  a.mm(b)    多维（包括二维）： a.matmul(b) == a@b    对应元素相乘： a.mul(b) == a*b
(4, 3, 28, 64) @ (4, 3, 64, 32) = (4, 3, 28, 32)  即前两维不变，后两维相乘
(4, 3, 28, 64) @ (4, 1, 64, 32) = (4, 3, 28, 32)  利用Broadcasting
(4, 3, 28, 64) @ (4, 64, 32)  出错，Broadcasting的4和3不对应
a.pow(i)  a的i次方

a.clamp(min[, max])  夹逼

a.norm(p, dim=0)    对0维上的对应位置的数据分别做p范数
in:  a = torch.full([5, 3], 1.)
     a.norm(2, dim=0)
out: tensor([2.2361, 2.2361, 2.2361])   sqrt(5)==2.2361 

a.min()  a.max()  a.mean()  a.prod()累乘   
a.sum()  a.argmax()最大值的索引  a.argmin()最小值的索引【铺成一维后的索引】
a.max(dim=0)返回0维上的最大值及索引    也可以加参数keepdim=True表示结果与a的形状一致
a.argmax(dim=0)  0维上的最大值的索引，a.argmin(dim=0)一样

a.topk(n, dim=1)  返回1维上的前n个最大值及其索引，添加参数largest = False求最小的
a.kthvalue(k, dim=1)  返回1维上的第k小的值及索引

in-place运算符：任何in-place运算符都以 _ 结尾，如x.add_(y)
    x.add(y) 不会影响x的值，但是x.add_(y)会将计算结果保存在x中。即执行z=x.add(y)与x.add_(y)之后，z的值与x的值相同

各种切片操作都支持，如 x[:, 1:] 包括所有行以及第一行以后的所有列

x = y.numpy()   将torch tensor  转为  numpy array       x与y共享内存
y = torch.from_numpy(x)    将torch tensor  转为  numpy array       x与y共享内存

torch.rand(1).item()  只包含一个元素的tensor，可以用item()函数将里面的value变为python类型的数值

高级函数

torch.where(condition, a, b) 根据条件选择赋值

condition = torch.rand(2, 2)   # tensor([[0.2886, 0.7170],
							#		  [0.4666, 0.5724]])
a = torch.zeros(2, 2)
b = torch.ones(2, 2)
c = torch.where(condition > 0.5, a, b)  # tensor([[1., 0.],
        					   	   	  #	        [1., 0.]])
# 相应位置，若condition在此处的值大于0.5则c在此处的值被赋予a的值，否则赋予b的值

torch.gather(input, dim, index) 在dim维度上按index的索引搜索input的值并返回


维度变换


x = x.view(3, 5)或x.reshape(3, 5) # 重塑形状，如同numpy中的reshape函数。可以将其中一个参数写为-1，解释器会自动推导。view和reshape本质不会改变数据在内存中的排列规则，只是改变了访问的规则"><meta name=author content="lu"><link rel=canonical href=https://ethereal-lu.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch%E5%AD%A6%E4%B9%A0/><link crossorigin=anonymous href=/assets/css/stylesheet.d72444526d7ecbdb0015438a7fa89054a658bf759d0542e2e5df81ce94b493ee.css integrity="sha256-1yREUm1+y9sAFUOKf6iQVKZYv3WdBULi5d+BzpS0k+4=" rel="preload stylesheet" as=style><link rel=icon href=https://ethereal-lu.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ethereal-lu.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ethereal-lu.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://ethereal-lu.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ethereal-lu.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ethereal-lu.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch%E5%AD%A6%E4%B9%A0/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://ethereal-lu.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch%E5%AD%A6%E4%B9%A0/"><meta property="og:site_name" content="lu"><meta property="og:title" content="PyTorch笔记"><meta property="og:description" content="torch的tensor默认为FloatTensor类型，可以使用torch.set_default_tensor_type(torch.DoubleTensor)设置默认类型为DoubleTensor，pytorch和numpy的张量在内存中都是默认按行优先顺序排列。
基础 生成张量 x = torch.empty(5, 3) == torch.Tensor(5, 3) # 构造一个未初始化的5✖3的矩阵，里面都是任意的随机数 y = torch.rand(5, 3) # 构造一个随机初始化的5✖3的矩阵，里面是从区间(0, 1)的均匀分布中抽取出的一组随机数 z = torch.randn(5, 3) # 构造一个随机初始化的5✖3的矩阵，里面是从标准正态分布中抽取出的一组随机数 a = torch.zeros(5, 3, dtype=torch.int) # 0填充，类型为整数，还可以是别的 torch.ones(5, 3) 1填充 torch.eye(3) 3×3的单位矩阵 b = torch.tensor([1, 2, 3, 4]) # 用数据直接构建向量 c = torch.randn_like(b, dtype=torch.float) # 构造形状与b相同的随机数组,torch.ones_like,torch.zeros_like,形状相同的全1.全0 torch.randint(1, 10, [5, 3]) # 构造一个5✖3的矩阵，里面是从[1, 10)中抽取出的一组随机整数 torch.normal(mean, std) # mean和std都是tensor，随机从mean为均值，std为方差的正太分布取值。输出形状为mean的形状。每一项一一对应，相互独立 torch.full([5, 3], 6) # 生成一个5✖3的矩阵，并将元素全部填充为6 torch.arange(0, 10) # tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) torch.arange(0, 10, 2) # tensor([0, 2, 4, 6, 8]) torch.linspace(0, 10, steps=4) # tensor([ 0.0000, 3.3333, 6.6667, 10.0000]) torch.logspace(0, 1, steps=10) # 对数函数中，y轴上在[0, 1]均匀取10个数，返回其对应的x轴上的值 常用函数及操作 idx = torch.randperm(4) a[idx] # idx == tensor([2, 0, 3, 1]) 打乱顺序。将a的第一维度按照idx的顺序重新排列，idx的顺序随机生成 a[i, j] 获取张量a中位于(i, j) 处的元素，类似于python中的a[i][j] a.dim() 返回张量的维度 c.shape == c.size() 都是返回数组形状，返回的是元组 list(c.shape) 将结果变为python中的list c.size(i) 返回上方所得元组中的第i个元素 c.numel() 返回各维度的乘积 x + y == torch.add(x, y) 两数组相加 乘法： 二维 a.mm(b) 多维（包括二维）： a.matmul(b) == a@b 对应元素相乘： a.mul(b) == a*b (4, 3, 28, 64) @ (4, 3, 64, 32) = (4, 3, 28, 32) 即前两维不变，后两维相乘 (4, 3, 28, 64) @ (4, 1, 64, 32) = (4, 3, 28, 32) 利用Broadcasting (4, 3, 28, 64) @ (4, 64, 32) 出错，Broadcasting的4和3不对应 a.pow(i) a的i次方 a.clamp(min[, max]) 夹逼 a.norm(p, dim=0) 对0维上的对应位置的数据分别做p范数 in: a = torch.full([5, 3], 1.) a.norm(2, dim=0) out: tensor([2.2361, 2.2361, 2.2361]) sqrt(5)==2.2361 a.min() a.max() a.mean() a.prod()累乘 a.sum() a.argmax()最大值的索引 a.argmin()最小值的索引【铺成一维后的索引】 a.max(dim=0)返回0维上的最大值及索引 也可以加参数keepdim=True表示结果与a的形状一致 a.argmax(dim=0) 0维上的最大值的索引，a.argmin(dim=0)一样 a.topk(n, dim=1) 返回1维上的前n个最大值及其索引，添加参数largest = False求最小的 a.kthvalue(k, dim=1) 返回1维上的第k小的值及索引 in-place运算符：任何in-place运算符都以 _ 结尾，如x.add_(y) x.add(y) 不会影响x的值，但是x.add_(y)会将计算结果保存在x中。即执行z=x.add(y)与x.add_(y)之后，z的值与x的值相同 各种切片操作都支持，如 x[:, 1:] 包括所有行以及第一行以后的所有列 x = y.numpy() 将torch tensor 转为 numpy array x与y共享内存 y = torch.from_numpy(x) 将torch tensor 转为 numpy array x与y共享内存 torch.rand(1).item() 只包含一个元素的tensor，可以用item()函数将里面的value变为python类型的数值 高级函数 torch.where(condition, a, b) 根据条件选择赋值 condition = torch.rand(2, 2) # tensor([[0.2886, 0.7170], #	[0.4666, 0.5724]]) a = torch.zeros(2, 2) b = torch.ones(2, 2) c = torch.where(condition > 0.5, a, b) # tensor([[1., 0.], #	[1., 0.]]) # 相应位置，若condition在此处的值大于0.5则c在此处的值被赋予a的值，否则赋予b的值 torch.gather(input, dim, index) 在dim维度上按index的索引搜索input的值并返回 维度变换 x = x.view(3, 5)或x.reshape(3, 5) # 重塑形状，如同numpy中的reshape函数。可以将其中一个参数写为-1，解释器会自动推导。view和reshape本质不会改变数据在内存中的排列规则，只是改变了访问的规则"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-07-01T17:45:10+00:00"><meta property="article:modified_time" content="2021-07-01T17:45:10+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="PyTorch笔记"><meta name=twitter:description content="torch的tensor默认为FloatTensor类型，可以使用torch.set_default_tensor_type(torch.DoubleTensor)设置默认类型为DoubleTensor，pytorch和numpy的张量在内存中都是默认按行优先顺序排列。

基础
生成张量
x = torch.empty(5, 3) == torch.Tensor(5, 3) # 构造一个未初始化的5✖3的矩阵，里面都是任意的随机数
y = torch.rand(5, 3)   # 构造一个随机初始化的5✖3的矩阵，里面是从区间(0, 1)的均匀分布中抽取出的一组随机数
z = torch.randn(5, 3)   # 构造一个随机初始化的5✖3的矩阵，里面是从标准正态分布中抽取出的一组随机数
a = torch.zeros(5, 3, dtype=torch.int)  # 0填充，类型为整数，还可以是别的     torch.ones(5, 3)  1填充  torch.eye(3) 3×3的单位矩阵
b = torch.tensor([1, 2, 3, 4])  # 用数据直接构建向量
c = torch.randn_like(b, dtype=torch.float)  # 构造形状与b相同的随机数组,torch.ones_like,torch.zeros_like,形状相同的全1.全0

torch.randint(1, 10, [5, 3])  # 构造一个5✖3的矩阵，里面是从[1, 10)中抽取出的一组随机整数
torch.normal(mean, std)   # mean和std都是tensor，随机从mean为均值，std为方差的正太分布取值。输出形状为mean的形状。每一项一一对应，相互独立
torch.full([5, 3], 6)   # 生成一个5✖3的矩阵，并将元素全部填充为6
torch.arange(0, 10)     # tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
torch.arange(0, 10, 2)  # tensor([0, 2, 4, 6, 8])
torch.linspace(0, 10, steps=4)  # tensor([ 0.0000,  3.3333,  6.6667, 10.0000])
torch.logspace(0, 1, steps=10)  # 对数函数中，y轴上在[0, 1]均匀取10个数，返回其对应的x轴上的值

常用函数及操作
idx = torch.randperm(4)   a[idx]    # idx == tensor([2, 0, 3, 1])   打乱顺序。将a的第一维度按照idx的顺序重新排列，idx的顺序随机生成

a[i, j] 获取张量a中位于(i, j) 处的元素，类似于python中的a[i][j]

a.dim() 返回张量的维度
c.shape == c.size()  都是返回数组形状，返回的是元组
list(c.shape)   将结果变为python中的list
c.size(i)   返回上方所得元组中的第i个元素
c.numel()   返回各维度的乘积

x + y == torch.add(x, y)   两数组相加
乘法： 二维  a.mm(b)    多维（包括二维）： a.matmul(b) == a@b    对应元素相乘： a.mul(b) == a*b
(4, 3, 28, 64) @ (4, 3, 64, 32) = (4, 3, 28, 32)  即前两维不变，后两维相乘
(4, 3, 28, 64) @ (4, 1, 64, 32) = (4, 3, 28, 32)  利用Broadcasting
(4, 3, 28, 64) @ (4, 64, 32)  出错，Broadcasting的4和3不对应
a.pow(i)  a的i次方

a.clamp(min[, max])  夹逼

a.norm(p, dim=0)    对0维上的对应位置的数据分别做p范数
in:  a = torch.full([5, 3], 1.)
     a.norm(2, dim=0)
out: tensor([2.2361, 2.2361, 2.2361])   sqrt(5)==2.2361 

a.min()  a.max()  a.mean()  a.prod()累乘   
a.sum()  a.argmax()最大值的索引  a.argmin()最小值的索引【铺成一维后的索引】
a.max(dim=0)返回0维上的最大值及索引    也可以加参数keepdim=True表示结果与a的形状一致
a.argmax(dim=0)  0维上的最大值的索引，a.argmin(dim=0)一样

a.topk(n, dim=1)  返回1维上的前n个最大值及其索引，添加参数largest = False求最小的
a.kthvalue(k, dim=1)  返回1维上的第k小的值及索引

in-place运算符：任何in-place运算符都以 _ 结尾，如x.add_(y)
    x.add(y) 不会影响x的值，但是x.add_(y)会将计算结果保存在x中。即执行z=x.add(y)与x.add_(y)之后，z的值与x的值相同

各种切片操作都支持，如 x[:, 1:] 包括所有行以及第一行以后的所有列

x = y.numpy()   将torch tensor  转为  numpy array       x与y共享内存
y = torch.from_numpy(x)    将torch tensor  转为  numpy array       x与y共享内存

torch.rand(1).item()  只包含一个元素的tensor，可以用item()函数将里面的value变为python类型的数值

高级函数

torch.where(condition, a, b) 根据条件选择赋值

condition = torch.rand(2, 2)   # tensor([[0.2886, 0.7170],
							#		  [0.4666, 0.5724]])
a = torch.zeros(2, 2)
b = torch.ones(2, 2)
c = torch.where(condition > 0.5, a, b)  # tensor([[1., 0.],
        					   	   	  #	        [1., 0.]])
# 相应位置，若condition在此处的值大于0.5则c在此处的值被赋予a的值，否则赋予b的值

torch.gather(input, dim, index) 在dim维度上按index的索引搜索input的值并返回


维度变换


x = x.view(3, 5)或x.reshape(3, 5) # 重塑形状，如同numpy中的reshape函数。可以将其中一个参数写为-1，解释器会自动推导。view和reshape本质不会改变数据在内存中的排列规则，只是改变了访问的规则"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://ethereal-lu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"PyTorch笔记","item":"https://ethereal-lu.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch%E5%AD%A6%E4%B9%A0/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"PyTorch笔记","name":"PyTorch笔记","description":"torch的tensor默认为FloatTensor类型，可以使用torch.set_default_tensor_type(torch.DoubleTensor)设置默认类型为DoubleTensor，pytorch和numpy的张量在内存中都是默认按行优先顺序排列。\n基础 生成张量 x = torch.empty(5, 3) == torch.Tensor(5, 3) # 构造一个未初始化的5✖3的矩阵，里面都是任意的随机数 y = torch.rand(5, 3) # 构造一个随机初始化的5✖3的矩阵，里面是从区间(0, 1)的均匀分布中抽取出的一组随机数 z = torch.randn(5, 3) # 构造一个随机初始化的5✖3的矩阵，里面是从标准正态分布中抽取出的一组随机数 a = torch.zeros(5, 3, dtype=torch.int) # 0填充，类型为整数，还可以是别的 torch.ones(5, 3) 1填充 torch.eye(3) 3×3的单位矩阵 b = torch.tensor([1, 2, 3, 4]) # 用数据直接构建向量 c = torch.randn_like(b, dtype=torch.float) # 构造形状与b相同的随机数组,torch.ones_like,torch.zeros_like,形状相同的全1.全0 torch.randint(1, 10, [5, 3]) # 构造一个5✖3的矩阵，里面是从[1, 10)中抽取出的一组随机整数 torch.normal(mean, std) # mean和std都是tensor，随机从mean为均值，std为方差的正太分布取值。输出形状为mean的形状。每一项一一对应，相互独立 torch.full([5, 3], 6) # 生成一个5✖3的矩阵，并将元素全部填充为6 torch.arange(0, 10) # tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) torch.arange(0, 10, 2) # tensor([0, 2, 4, 6, 8]) torch.linspace(0, 10, steps=4) # tensor([ 0.0000, 3.3333, 6.6667, 10.0000]) torch.logspace(0, 1, steps=10) # 对数函数中，y轴上在[0, 1]均匀取10个数，返回其对应的x轴上的值 常用函数及操作 idx = torch.randperm(4) a[idx] # idx == tensor([2, 0, 3, 1]) 打乱顺序。将a的第一维度按照idx的顺序重新排列，idx的顺序随机生成 a[i, j] 获取张量a中位于(i, j) 处的元素，类似于python中的a[i][j] a.dim() 返回张量的维度 c.shape == c.size() 都是返回数组形状，返回的是元组 list(c.shape) 将结果变为python中的list c.size(i) 返回上方所得元组中的第i个元素 c.numel() 返回各维度的乘积 x + y == torch.add(x, y) 两数组相加 乘法： 二维 a.mm(b) 多维（包括二维）： a.matmul(b) == a@b 对应元素相乘： a.mul(b) == a*b (4, 3, 28, 64) @ (4, 3, 64, 32) = (4, 3, 28, 32) 即前两维不变，后两维相乘 (4, 3, 28, 64) @ (4, 1, 64, 32) = (4, 3, 28, 32) 利用Broadcasting (4, 3, 28, 64) @ (4, 64, 32) 出错，Broadcasting的4和3不对应 a.pow(i) a的i次方 a.clamp(min[, max]) 夹逼 a.norm(p, dim=0) 对0维上的对应位置的数据分别做p范数 in: a = torch.full([5, 3], 1.) a.norm(2, dim=0) out: tensor([2.2361, 2.2361, 2.2361]) sqrt(5)==2.2361 a.min() a.max() a.mean() a.prod()累乘 a.sum() a.argmax()最大值的索引 a.argmin()最小值的索引【铺成一维后的索引】 a.max(dim=0)返回0维上的最大值及索引 也可以加参数keepdim=True表示结果与a的形状一致 a.argmax(dim=0) 0维上的最大值的索引，a.argmin(dim=0)一样 a.topk(n, dim=1) 返回1维上的前n个最大值及其索引，添加参数largest = False求最小的 a.kthvalue(k, dim=1) 返回1维上的第k小的值及索引 in-place运算符：任何in-place运算符都以 _ 结尾，如x.add_(y) x.add(y) 不会影响x的值，但是x.add_(y)会将计算结果保存在x中。即执行z=x.add(y)与x.add_(y)之后，z的值与x的值相同 各种切片操作都支持，如 x[:, 1:] 包括所有行以及第一行以后的所有列 x = y.numpy() 将torch tensor 转为 numpy array x与y共享内存 y = torch.from_numpy(x) 将torch tensor 转为 numpy array x与y共享内存 torch.rand(1).item() 只包含一个元素的tensor，可以用item()函数将里面的value变为python类型的数值 高级函数 torch.where(condition, a, b) 根据条件选择赋值 condition = torch.rand(2, 2) # tensor([[0.2886, 0.7170], #\t[0.4666, 0.5724]]) a = torch.zeros(2, 2) b = torch.ones(2, 2) c = torch.where(condition \u0026gt; 0.5, a, b) # tensor([[1., 0.], #\t[1., 0.]]) # 相应位置，若condition在此处的值大于0.5则c在此处的值被赋予a的值，否则赋予b的值 torch.gather(input, dim, index) 在dim维度上按index的索引搜索input的值并返回 维度变换 x = x.view(3, 5)或x.reshape(3, 5) # 重塑形状，如同numpy中的reshape函数。可以将其中一个参数写为-1，解释器会自动推导。view和reshape本质不会改变数据在内存中的排列规则，只是改变了访问的规则\n","keywords":[],"articleBody":"torch的tensor默认为FloatTensor类型，可以使用torch.set_default_tensor_type(torch.DoubleTensor)设置默认类型为DoubleTensor，pytorch和numpy的张量在内存中都是默认按行优先顺序排列。\n基础 生成张量 x = torch.empty(5, 3) == torch.Tensor(5, 3) # 构造一个未初始化的5✖3的矩阵，里面都是任意的随机数 y = torch.rand(5, 3) # 构造一个随机初始化的5✖3的矩阵，里面是从区间(0, 1)的均匀分布中抽取出的一组随机数 z = torch.randn(5, 3) # 构造一个随机初始化的5✖3的矩阵，里面是从标准正态分布中抽取出的一组随机数 a = torch.zeros(5, 3, dtype=torch.int) # 0填充，类型为整数，还可以是别的 torch.ones(5, 3) 1填充 torch.eye(3) 3×3的单位矩阵 b = torch.tensor([1, 2, 3, 4]) # 用数据直接构建向量 c = torch.randn_like(b, dtype=torch.float) # 构造形状与b相同的随机数组,torch.ones_like,torch.zeros_like,形状相同的全1.全0 torch.randint(1, 10, [5, 3]) # 构造一个5✖3的矩阵，里面是从[1, 10)中抽取出的一组随机整数 torch.normal(mean, std) # mean和std都是tensor，随机从mean为均值，std为方差的正太分布取值。输出形状为mean的形状。每一项一一对应，相互独立 torch.full([5, 3], 6) # 生成一个5✖3的矩阵，并将元素全部填充为6 torch.arange(0, 10) # tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) torch.arange(0, 10, 2) # tensor([0, 2, 4, 6, 8]) torch.linspace(0, 10, steps=4) # tensor([ 0.0000, 3.3333, 6.6667, 10.0000]) torch.logspace(0, 1, steps=10) # 对数函数中，y轴上在[0, 1]均匀取10个数，返回其对应的x轴上的值 常用函数及操作 idx = torch.randperm(4) a[idx] # idx == tensor([2, 0, 3, 1]) 打乱顺序。将a的第一维度按照idx的顺序重新排列，idx的顺序随机生成 a[i, j] 获取张量a中位于(i, j) 处的元素，类似于python中的a[i][j] a.dim() 返回张量的维度 c.shape == c.size() 都是返回数组形状，返回的是元组 list(c.shape) 将结果变为python中的list c.size(i) 返回上方所得元组中的第i个元素 c.numel() 返回各维度的乘积 x + y == torch.add(x, y) 两数组相加 乘法： 二维 a.mm(b) 多维（包括二维）： a.matmul(b) == a@b 对应元素相乘： a.mul(b) == a*b (4, 3, 28, 64) @ (4, 3, 64, 32) = (4, 3, 28, 32) 即前两维不变，后两维相乘 (4, 3, 28, 64) @ (4, 1, 64, 32) = (4, 3, 28, 32) 利用Broadcasting (4, 3, 28, 64) @ (4, 64, 32) 出错，Broadcasting的4和3不对应 a.pow(i) a的i次方 a.clamp(min[, max]) 夹逼 a.norm(p, dim=0) 对0维上的对应位置的数据分别做p范数 in: a = torch.full([5, 3], 1.) a.norm(2, dim=0) out: tensor([2.2361, 2.2361, 2.2361]) sqrt(5)==2.2361 a.min() a.max() a.mean() a.prod()累乘 a.sum() a.argmax()最大值的索引 a.argmin()最小值的索引【铺成一维后的索引】 a.max(dim=0)返回0维上的最大值及索引 也可以加参数keepdim=True表示结果与a的形状一致 a.argmax(dim=0) 0维上的最大值的索引，a.argmin(dim=0)一样 a.topk(n, dim=1) 返回1维上的前n个最大值及其索引，添加参数largest = False求最小的 a.kthvalue(k, dim=1) 返回1维上的第k小的值及索引 in-place运算符：任何in-place运算符都以 _ 结尾，如x.add_(y) x.add(y) 不会影响x的值，但是x.add_(y)会将计算结果保存在x中。即执行z=x.add(y)与x.add_(y)之后，z的值与x的值相同 各种切片操作都支持，如 x[:, 1:] 包括所有行以及第一行以后的所有列 x = y.numpy() 将torch tensor 转为 numpy array x与y共享内存 y = torch.from_numpy(x) 将torch tensor 转为 numpy array x与y共享内存 torch.rand(1).item() 只包含一个元素的tensor，可以用item()函数将里面的value变为python类型的数值 高级函数 torch.where(condition, a, b) 根据条件选择赋值 condition = torch.rand(2, 2) # tensor([[0.2886, 0.7170], #\t[0.4666, 0.5724]]) a = torch.zeros(2, 2) b = torch.ones(2, 2) c = torch.where(condition \u003e 0.5, a, b) # tensor([[1., 0.], #\t[1., 0.]]) # 相应位置，若condition在此处的值大于0.5则c在此处的值被赋予a的值，否则赋予b的值 torch.gather(input, dim, index) 在dim维度上按index的索引搜索input的值并返回 维度变换 x = x.view(3, 5)或x.reshape(3, 5) # 重塑形状，如同numpy中的reshape函数。可以将其中一个参数写为-1，解释器会自动推导。view和reshape本质不会改变数据在内存中的排列规则，只是改变了访问的规则\nunsqueeze(index) # 展开，即在index（从0开始）的位置插入一个维度，index可以为负数表示从后往前数（在之后插）\nsqueeze(index) # 挤压，即在index（从0开始）的位置删除一个维度，index可以为负数表示从后往前数，当不传参时，会将所有大小为1的维度全部删除，当index位置的维度大小不为1时，不挤压，原样返回\n上方的a.unsqueeze(index)和a.squeeze(index)都不会改变a的形状，只是返回一个改变后的形状\nexpand # 维度扩张，若b之前形状为(1, 32, 1, 1) 则执行b.expand(4, 32, 14, 14)后形状变为(4, 32, 14, 14).执行b.expand(-1, 32, 14, -1)后形状变为(1, 32, 14, 1)，即传入-1的位置不做改变。repeat和expand的功能一样，但是repeat会立即在内存中拷贝所有数据，浪费内存，故不推荐使用，expand为懒复制\na.expand_as(b) # 将a扩展为b的形状\nt() # b.t() 转置，只适用于二维\ntranspose # 交换两个维度的顺序，若b之前形状为(4, 32, 14, 14) 则执行b.transpose(1, 3)后形状变为(4, 14, 14, 32)\npermute # 交换所有维度的顺序，若b之前形状为(4, 32, 14, 14) 则执行b.permute(2, 3, 1, 0)后形状变为(14, 14, 32, 4)\ntranspose或permute与view一起使用时应注意的点：transpose或permute同view一样不会改变数据在内存中的排列规则，只是改变了访问的规则。但执行transpose或permute后数据访问顺序发生改变，与内存的存储顺序不一致，此时再执行view方法会发生混乱，因此需要在view方法之前执行contiguous()。contiguous()意思是连续化，即将访问顺序连续但内存位置不连续的数据，拷贝使其内存位置也连续。contiguous()方法首先拷贝了一份原始张量在内存中的地址，然后将地址按照执行transpose或permute之后的形状进行排列。之后view对新的地址改变访问规则。\nBroadcasting(自动扩展) Broadcasting是unsqueeze和expand的组合，由pytorch自动完成，无需手动扩展。 以（4， 32， 8）为例，左边的4为大维度，右边的8为小维度。Broadcasting是从小维度向大维度扩展的，即每次都是unsqueeze(0)。 若A和B的维度不同，则执行A + B需满足Broadcasting的条件：B中已经定义了的小维度的size要么为1要么与A的小维度size相等。若A的size为（4， 32， 8），则B的可能形状为(1), (8), (1, 1), (1, 8), (32, 1), (32, 8), (1, 1, 1), (1, 1, 8)等，其他的形状都不满足Broadcasting的条件。 Broadcasting不是函数，不需要手动调用，在执行形状不同的tensor相加时会自动实现，例如想要在5×3的a上全部加5，则只需a+5即可 拼接与拆分 torch.cat([a, b], dim=0) # 在0维上对a和b拼接，即若a的形状为(4, 2, 6),b的形状为(3, 2, 6),则拼接后形状为(7, 2, 6), 即要保持其他维度的size相等 torch.stack([a, b], dim=0) # 在0维前插一个size为2的维度，即若a和b的形状都为(32, 8),则拼接后形状为(2, 32, 8)。stack要求a和b的形状完全一样，其中新形状的(0, 32, 8) == a; (1, 32, 8) == b。 split() 拆分为指定份额 a = torch.randn(3, 32, 8) a1, a2 = a.split([2, 1], dim=0) # 在0维上拆分，将3拆分为2份和1份 a1.shape, a2.shape # out: (torch.Size([2, 32, 8]), torch.Size([1, 32, 8])) a1, a2, a3= a.split(1, dim=0) # 在0维上拆分，1表示每份的份额为1 a1.shape, a2.shape, a3.shape # out: (torch.Size([1, 32, 8]), torch.Size([1, 32, 8]), torch.Size([1, 32, 8])) chunk() 拆分为指定份数 a1, a2 = a.split(2, dim=0) # 在0维上拆分，2表示共拆分为2份 a1.shape, a2.shape # out: (torch.Size([2, 32, 8]), torch.Size([1, 32, 8])) 使用.to()方法可以将张量移动到GPU上计算： if torch.cuda.is_available(): device = torch.device(\"cuda\") # 指定设备为英伟达GPU y = torch.ones_like(x, device=device) # 还可以f = torch.rand(4, 3, device=torch.device(\"cuda\")) x = x.to(device) z = x + y print(z.to(\"cpu\", torch.int)) # 还可以再转回cpu中. z.to(\"cpu\") == z.cpu() 动量 package torch.optim class SGD(Optimizer): def __init__(self, params, lr=required, momentum=0, dampening=0, weight_decay=0, nesterov=False) # 上方构造函数的参数中： # params：模型的参数 # momentum：动量，即上一次梯度的权重 # weight_decay：L2正则化的权重 # Adam方法已经自动处理了动量相关 学习率衰减 learning_rate刚开始较大，加快收敛速率。之后逐渐减小，避免震荡。\n# 第一种方法：监视loss，若发现loss多次不变，则减小learning_rate optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # 实例化 scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min') for t in range(500): '''forward pass''' y_pred = model(x) '''compute loss''' loss = loss_fn(y_pred, y) scheduler.step(loss) # 监视loss，若发现loss多次不变，则减小learning_rate ------------------------------------------------------------------------------------------- # 第二种方法：指定一定的epoch次数后减小learning_rate optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # 指定每100次epoch，learning_rate = learning_rate * 0.5 scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=100, gamma=0.5) for t in range(500): scheduler.step() '''forward pass''' y_pred = model(x) '''compute loss''' loss = loss_fn(y_pred, y) 卷积神经网络 卷积操作 torch.nn.Conv2d torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros') # in_channels:输入图片通道数 # out_channels：输出的通道数，即过滤器（核）的个数 ✖ 输入通道数 示例\nlayer = torch.nn.Conv2d(1, 3, kernel_size = 3, stride=1, padding=0) x = torch.rand(1, 1, 28, 28) # 模拟图片 out = layer.forward(x) # 不推荐直接使用 forward 函数 out.shape # torch.Size([1, 3, 26, 26]) ------------------------------------------------------------------- out = layer(x) # 推荐用这个。调用 __call__ ，封装了 hooks 和 forward 。作用与 forward 相同。 out.shape # torch.Size([1, 3, 26, 26]) torch.nn.functional.conv2d 函数也可以实现卷积操作，但是不推荐。\n池化操作 torch.nn.MaxPool2d 池化 就是 降采样，将原本图像缩小。分为 最大池化 和 平均池化\ntorch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) # kernel_size：核窗口大小 # stride： 步长 示例\npooling = torch.nn.MaxPool2d(2, stride=2) # 最大池化 res = pooling(out) res.shape # torch.Size([1, 3, 13, 13]) torch.nn.functional.avg_pool2d 函数也可以实现池化操作，但是不推荐。\n上采样–将原本图像放大 torch.nn.functional.interpolate\ntorch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None) # input (Tensor) – the input tensor 需要放大的图像 # scale_factor ： 放大倍数 # mode ： 放大的方法。nearest 表示用最近的像素点来填充放大后周围的像素点 ReLU\ntorch.nn.ReLU(inplace=False) # inplace： 即 in-place 运算符，将运算结果保存在自身。默认为False。一般设为True，减少内存消耗 标准化 Batch Norm 假设使用 sigmoid 函数作为激活函数，则 当数据量偏大或偏小时会出现梯度消失。故应将数据标准化为标准分布，这样会使数据有好的收敛效果且模型更稳定。（训练和测试要切换）\ntorch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) # num_features：图像的通道数 示例\ninput = torch.rand(100, 16, 784) # 28 * 28 == 784 layer = torch.nn.BatchNorm1d(16) # 也可以不压缩，用二维的方法 out = layer(input) layer.running_mean # 均值 # tensor([0.0499, 0.0500, 0.0500, 0.0500, 0.0501, 0.0499, 0.0500, 0.0501, 0.0501, # 0.0500, 0.0501, 0.0502, 0.0498, 0.0501, 0.0500, 0.0500]) layer.running_var # 方差 # tensor([0.9083, 0.9083, 0.9083, 0.9084, 0.9083, 0.9084, 0.9083, 0.9083, 0.9083, # 0.9083, 0.9084, 0.9083, 0.9083, 0.9083, 0.9084, 0.9083]) 标准化过程：\n求一个通道内的所有像素的 均值 E[x] 和 方差 Var[x] 用公式标准化 之后迭代更新 γ 和 β 残差神经网络 普通神经网络在层数达到二十层时，会达到一个瓶颈，即层数继续叠加，效果反而变差。\n残差神经网络在浅层与深层之间增加了一个短路直连，当深层的效果更好时，就选择该值，若更差就退化为浅层神经网络。使得深层网络最差也不会差于浅层神经网络。\n示例：\nclass ResBlk(nn.Module): def __init__(self, in_ch, out_ch): super(ResBlk, self).__init__() self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(out_ch) self.relu = nn.ReLU() self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(out_ch) self.extra = nn.Sequential() # 若输入通道等于输出通道，则直连 if out_ch != in_ch: self.extra = nn.Sequential( # 若输入通道不等于输出通道，则用1 × 1的核函数使其相等 nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1), nn.BatchNorm2d(out_ch) ) def forward(self, x): out = self.relu(self.bn1(self.conv1(x))) out = self.bn2(self.conv2(out)) # 完成一个单元你的输出 out = self.extra(x) + out # 直连输出与单元输出相加 return out 铺平 torch.nn.Flatten\n卷积神经网络的后边会有一个全连接层，前面的卷积池化操作是二维的，而全连接层是一维的，故中间需要将二维铺平为一维。\nclass Flatten(nn.Module): def __init__(self): super(Flatten, self).__init__() def forward(self, input): return input.view(input.size(0), -1) 线性容器 torch.nn.Sequential\n按添加的顺序依次执行\nclass TestNet(nn.Module): def __init__(self): super(TestNet, self).__init__() self.net = nn.Sequential( nn.Conv2d(1, 16, stride=1, padding=1), nn.MaxPool2d(2, 2), Flatten(), # 即上方定义的Flatten nn.Linear(1 * 14 * 14, 10) ) def forward(self, x): return self.net(x) # 按线性容器中的顺序执行 参数包装 torch.nn.parameter.Parameter\n参数通过该类包装后会自动添加到模型的参数列表中，即model.parameters()。自动求导迭代优化。\nclass MyLinear(nn.Module): def __init__(self, in, out): super(MyLinear, self).__init__() self.w = nn.parameter.Parameter(torch.randn(out, in)) self.b = nn.parameter.Parameter(torch.randn(out)) def forward(self, x): x = x @ self.w.t() + self.b return x 数据增强\n通过翻转、旋转、裁剪、缩放等方法，把原来少量的数据集扩大。\n循环神经网络 GloVe 词向量\nfrom torchnlp.word_to_vector import GloVe vectors = GloVe() # 全局词向量。词与其向量组成的dict vectors['hello'] # 输出该词的向量 torch.nn.RNN torch.nn.RNN参数的详解\n数据如何传入RNN网络\nimport torch from torch import nn # 表示feature_len=100, hidden_len=20, 层数=1 rnn = nn.RNN(100, 20, 1) # 输入3个样本序列(batch=3), 序列长为10(seq_len=10), 每个特征100维度(feature_len=100) # 即有3句话，每句话的单词数为10，每个单词用100维向量表示 x = torch.randn(10, 3, 100) # 传入RNN处理, 另外传入h_0, shape是\u003c层数, batch, hidden_len=20\u003e out, h = rnn(x, torch.zeros(1, 3, 20)) # 输出返回的out和最终的隐藏记忆单元的shape print(out.shape) # torch.Size([10, 3, 20]) print(h.shape) # torch.Size([1, 3, 20]) 对于一个RNN单元的输出 h~t ，乘以权重矩阵W变为out，乘以权重矩阵V变为下一个时间节点的输入。但是一般好像不存在W和V，因此两个输入是同一个东西。\nLSTM LSTM在原理上通过三道门控制当前单词与过去单词的权重，在pytorch实现上面仅是增加了一个参数 c 来表示过去的记忆。\n","wordCount":"1074","inLanguage":"en","datePublished":"2021-07-01T17:45:10Z","dateModified":"2021-07-01T17:45:10Z","author":{"@type":"Person","name":"lu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ethereal-lu.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch%E5%AD%A6%E4%B9%A0/"},"publisher":{"@type":"Organization","name":"lu","logo":{"@type":"ImageObject","url":"https://ethereal-lu.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ethereal-lu.github.io/ accesskey=h title="lu (Alt + H)">lu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://ethereal-lu.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://ethereal-lu.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ethereal-lu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://ethereal-lu.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">PyTorch笔记</h1><div class=post-meta><span title='2021-07-01 17:45:10 +0000 UTC'>2021-07-01</span>&nbsp;·&nbsp;1074 words&nbsp;·&nbsp;lu</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#基础>基础</a><ul><li><a href=#生成张量>生成张量</a></li><li><a href=#常用函数及操作>常用函数及操作</a></li><li><a href=#高级函数>高级函数</a></li><li><a href=#维度变换>维度变换</a></li><li><a href=#broadcasting自动扩展>Broadcasting(自动扩展)</a></li><li><a href=#拼接与拆分>拼接与拆分</a></li><li><a href=#使用to方法可以将张量移动到gpu上计算>使用.to()方法可以将张量移动到GPU上计算：</a></li><li><a href=#动量>动量</a></li><li><a href=#学习率衰减>学习率衰减</a></li></ul></li><li><a href=#卷积神经网络>卷积神经网络</a><ul><li><a href=#卷积操作---torchnnconv2d>卷积操作 torch.nn.Conv2d</a></li><li><a href=#池化操作---torchnnmaxpool2d>池化操作 torch.nn.MaxPool2d</a></li><li><a href=#标准化-batch-norm>标准化 Batch Norm</a></li><li><a href=#残差神经网络>残差神经网络</a></li></ul></li><li><a href=#循环神经网络>循环神经网络</a><ul><li><a href=#torchnnrnn>torch.nn.RNN</a></li><li><a href=#lstm>LSTM</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>torch的tensor默认为FloatTensor类型，可以使用<code>torch.set_default_tensor_type(torch.DoubleTensor)</code>设置默认类型为DoubleTensor，pytorch和numpy的张量在内存中都是默认按行优先顺序排列。</p><hr><h2 id=基础>基础<a hidden class=anchor aria-hidden=true href=#基础>#</a></h2><h3 id=生成张量>生成张量<a hidden class=anchor aria-hidden=true href=#生成张量>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>empty(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>3</span>) <span style=color:#f92672>==</span> torch<span style=color:#f92672>.</span>Tensor(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>3</span>) <span style=color:#75715e># 构造一个未初始化的5✖3的矩阵，里面都是任意的随机数</span>
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>3</span>)   <span style=color:#75715e># 构造一个随机初始化的5✖3的矩阵，里面是从区间(0, 1)的均匀分布中抽取出的一组随机数</span>
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>3</span>)   <span style=color:#75715e># 构造一个随机初始化的5✖3的矩阵，里面是从标准正态分布中抽取出的一组随机数</span>
</span></span><span style=display:flex><span>a <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>3</span>, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>int)  <span style=color:#75715e># 0填充，类型为整数，还可以是别的     torch.ones(5, 3)  1填充  torch.eye(3) 3×3的单位矩阵</span>
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>])  <span style=color:#75715e># 用数据直接构建向量</span>
</span></span><span style=display:flex><span>c <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn_like(b, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)  <span style=color:#75715e># 构造形状与b相同的随机数组,torch.ones_like,torch.zeros_like,形状相同的全1.全0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>10</span>, [<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>3</span>])  <span style=color:#75715e># 构造一个5✖3的矩阵，里面是从[1, 10)中抽取出的一组随机整数</span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>normal(mean, std)   <span style=color:#75715e># mean和std都是tensor，随机从mean为均值，std为方差的正太分布取值。输出形状为mean的形状。每一项一一对应，相互独立</span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>full([<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>3</span>], <span style=color:#ae81ff>6</span>)   <span style=color:#75715e># 生成一个5✖3的矩阵，并将元素全部填充为6</span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>10</span>)     <span style=color:#75715e># tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>2</span>)  <span style=color:#75715e># tensor([0, 2, 4, 6, 8])</span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>linspace(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>10</span>, steps<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>)  <span style=color:#75715e># tensor([ 0.0000,  3.3333,  6.6667, 10.0000])</span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>logspace(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, steps<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)  <span style=color:#75715e># 对数函数中，y轴上在[0, 1]均匀取10个数，返回其对应的x轴上的值</span>
</span></span></code></pre></div><hr><h3 id=常用函数及操作>常用函数及操作<a hidden class=anchor aria-hidden=true href=#常用函数及操作>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#ae81ff>idx = torch.randperm(4)   a[idx]   </span> <span style=color:#75715e># idx == tensor([2, 0, 3, 1])   打乱顺序。将a的第一维度按照idx的顺序重新排列，idx的顺序随机生成</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>a[i, j] 获取张量a中位于(i, j) 处的元素，类似于python中的a[i][j]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>a.dim() 返回张量的维度</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>c.shape == c.size()  都是返回数组形状，返回的是元组</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>list(c.shape)   将结果变为python中的list</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>c.size(i)   返回上方所得元组中的第i个元素</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>c.numel()   返回各维度的乘积</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>x + y == torch.add(x, y)   两数组相加</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>乘法： 二维  a.mm(b)    多维（包括二维）： a.matmul(b) == a@b    对应元素相乘： a.mul(b) == a*b</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>(4, 3, 28, 64) @ (4, 3, 64, 32) = (4, 3, 28, 32)  即前两维不变，后两维相乘</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>(4, 3, 28, 64) @ (4, 1, 64, 32) = (4, 3, 28, 32)  利用Broadcasting</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>(4, 3, 28, 64) @ (4, 64, 32)  出错，Broadcasting的4和3不对应</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>a.pow(i)  a的i次方</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>a.clamp(min[, max])  夹逼</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>a.norm(p, dim=0)    对0维上的对应位置的数据分别做p范数</span>
</span></span><span style=display:flex><span><span style=color:#f92672>in</span>:  <span style=color:#ae81ff>a = torch.full([5, 3], 1.)</span>
</span></span><span style=display:flex><span>     <span style=color:#ae81ff>a.norm(2, dim=0)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>out</span>: <span style=color:#ae81ff>tensor([2.2361, 2.2361, 2.2361])   sqrt(5)==2.2361 </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>a.min()  a.max()  a.mean()  a.prod()累乘   </span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>a.sum()  a.argmax()最大值的索引  a.argmin()最小值的索引【铺成一维后的索引】</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>a.max(dim=0)返回0维上的最大值及索引    也可以加参数keepdim=True表示结果与a的形状一致</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>a.argmax(dim=0)  0维上的最大值的索引，a.argmin(dim=0)一样</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>a.topk(n, dim=1)  返回1维上的前n个最大值及其索引，添加参数largest = False求最小的</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>a.kthvalue(k, dim=1)  返回1维上的第k小的值及索引</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>in-place运算符：任何in-place运算符都以 _ 结尾，如x.add_(y)</span>
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>x.add(y) 不会影响x的值，但是x.add_(y)会将计算结果保存在x中。即执行z=x.add(y)与x.add_(y)之后，z的值与x的值相同</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>各种切片操作都支持，如 x[:, 1:] 包括所有行以及第一行以后的所有列</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>x = y.numpy()   将torch tensor  转为  numpy array       x与y共享内存</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>y</span> <span style=color:#ae81ff>= torch.from_numpy(x)    将torch tensor  转为  numpy array       x与y共享内存</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>torch.rand(1).item()  只包含一个元素的tensor，可以用item()函数将里面的value变为python类型的数值</span>
</span></span></code></pre></div><hr><h3 id=高级函数>高级函数<a hidden class=anchor aria-hidden=true href=#高级函数>#</a></h3><ul><li><strong>torch.where(condition, a, b)</strong> 根据条件选择赋值</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>condition <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)   <span style=color:#75715e># tensor([[0.2886, 0.7170],</span>
</span></span><span style=display:flex><span>							<span style=color:#75715e>#		  [0.4666, 0.5724]])</span>
</span></span><span style=display:flex><span>a <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>ones(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>c <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>where(condition <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0.5</span>, a, b)  <span style=color:#75715e># tensor([[1., 0.],</span>
</span></span><span style=display:flex><span>        					   	   	  <span style=color:#75715e>#	        [1., 0.]])</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 相应位置，若condition在此处的值大于0.5则c在此处的值被赋予a的值，否则赋予b的值</span>
</span></span></code></pre></div><ul><li><strong>torch.gather(input, dim, index)</strong> 在dim维度上按index的索引搜索input的值并返回</li></ul><hr><h3 id=维度变换>维度变换<a hidden class=anchor aria-hidden=true href=#维度变换>#</a></h3><ul><li><p>x = x.view(3, 5)或x.reshape(3, 5) # 重塑形状，如同numpy中的reshape函数。可以将其中一个参数写为-1，解释器会自动推导。view和reshape本质不会改变数据在内存中的排列规则，只是改变了访问的规则</p></li><li><p>unsqueeze(index) # 展开，即在index（从0开始）的位置插入一个维度，index可以为负数表示从后往前数（在之后插）</p></li><li><p>squeeze(index) # 挤压，即在index（从0开始）的位置删除一个维度，index可以为负数表示从后往前数，当不传参时，会将所有大小为1的维度全部删除，当index位置的维度大小不为1时，不挤压，原样返回</p></li><li><p>上方的a.unsqueeze(index)和a.squeeze(index)都不会改变a的形状，只是返回一个改变后的形状</p></li><li><p>expand # 维度扩张，若b之前形状为(1, 32, 1, 1) 则执行b.expand(4, 32, 14, 14)后形状变为(4, 32, 14, 14).执行b.expand(-1, 32, 14, -1)后形状变为(1, 32, 14, 1)，即传入-1的位置不做改变。repeat和expand的功能一样，但是repeat会立即在内存中拷贝所有数据，浪费内存，故不推荐使用，expand为懒复制</p></li><li><p>a.expand_as(b) # 将a扩展为b的形状</p></li><li><p>t() # b.t() 转置，只适用于二维</p></li><li><p>transpose # 交换两个维度的顺序，若b之前形状为(4, 32, 14, 14) 则执行b.transpose(1, 3)后形状变为(4, 14, 14, 32)</p></li><li><p>permute # 交换所有维度的顺序，若b之前形状为(4, 32, 14, 14) 则执行b.permute(2, 3, 1, 0)后形状变为(14, 14, 32, 4)</p></li><li><p>transpose或permute与view一起使用时应注意的点：transpose或permute同view一样不会改变数据在内存中的排列规则，只是改变了访问的规则。但执行transpose或permute后数据访问顺序发生改变，与内存的存储顺序不一致，此时再执行view方法会发生混乱，因此需要在view方法之前执行contiguous()。contiguous()意思是连续化，即将访问顺序连续但内存位置不连续的数据，拷贝使其内存位置也连续。contiguous()方法首先拷贝了一份原始张量在内存中的地址，然后将地址按照执行transpose或permute之后的形状进行排列。之后view对新的地址改变访问规则。</p></li></ul><hr><h3 id=broadcasting自动扩展>Broadcasting(自动扩展)<a hidden class=anchor aria-hidden=true href=#broadcasting自动扩展>#</a></h3><ul><li>Broadcasting是unsqueeze和expand的组合，由pytorch自动完成，无需手动扩展。</li><li>以（4， 32， 8）为例，左边的4为大维度，右边的8为小维度。Broadcasting是从小维度向大维度扩展的，即每次都是unsqueeze(0)。</li><li>若A和B的维度不同，则执行A + B需满足Broadcasting的条件：B中已经定义了的小维度的size要么为1要么与A的小维度size相等。若A的size为（4， 32， 8），则B的可能形状为(1), (8), (1, 1), (1, 8), (32, 1), (32, 8), (1, 1, 1), (1, 1, 8)等，其他的形状都不满足Broadcasting的条件。</li><li>Broadcasting不是函数，不需要手动调用，在执行形状不同的tensor相加时会自动实现，例如想要在5×3的a上全部加5，则只需a+5即可</li></ul><hr><h3 id=拼接与拆分>拼接与拆分<a hidden class=anchor aria-hidden=true href=#拼接与拆分>#</a></h3><ul><li>torch.cat([a, b], dim=0) # 在0维上对a和b拼接，即若a的形状为(4, 2, 6),b的形状为(3, 2, 6),则拼接后形状为(7, 2, 6), 即要保持其他维度的size相等</li><li>torch.stack([a, b], dim=0) # 在0维前插一个size为2的维度，即若a和b的形状都为(32, 8),则拼接后形状为(2, 32, 8)。stack要求a和b的形状完全一样，其中新形状的(0, 32, 8) == a; (1, 32, 8) == b。</li><li>split() 拆分为指定份额</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>a <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>a1, a2 <span style=color:#f92672>=</span> a<span style=color:#f92672>.</span>split([<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>], dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)   <span style=color:#75715e># 在0维上拆分，将3拆分为2份和1份</span>
</span></span><span style=display:flex><span>a1<span style=color:#f92672>.</span>shape, a2<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span><span style=color:#75715e># out: (torch.Size([2, 32, 8]), torch.Size([1, 32, 8]))</span>
</span></span><span style=display:flex><span>a1, a2, a3<span style=color:#f92672>=</span> a<span style=color:#f92672>.</span>split(<span style=color:#ae81ff>1</span>, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)     <span style=color:#75715e># 在0维上拆分，1表示每份的份额为1</span>
</span></span><span style=display:flex><span>a1<span style=color:#f92672>.</span>shape, a2<span style=color:#f92672>.</span>shape, a3<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span><span style=color:#75715e># out: (torch.Size([1, 32, 8]), torch.Size([1, 32, 8]), torch.Size([1, 32, 8]))</span>
</span></span></code></pre></div><ul><li>chunk() 拆分为指定份数</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>a1, a2 <span style=color:#f92672>=</span> a<span style=color:#f92672>.</span>split(<span style=color:#ae81ff>2</span>, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)        <span style=color:#75715e># 在0维上拆分，2表示共拆分为2份</span>
</span></span><span style=display:flex><span>a1<span style=color:#f92672>.</span>shape, a2<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span><span style=color:#75715e># out: (torch.Size([2, 32, 8]), torch.Size([1, 32, 8]))</span>
</span></span></code></pre></div><hr><h3 id=使用to方法可以将张量移动到gpu上计算>使用.to()方法可以将张量移动到GPU上计算：<a hidden class=anchor aria-hidden=true href=#使用to方法可以将张量移动到gpu上计算>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available():
</span></span><span style=display:flex><span>    device <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>device(<span style=color:#e6db74>&#34;cuda&#34;</span>)    <span style=color:#75715e># 指定设备为英伟达GPU</span>
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>ones_like(x, device<span style=color:#f92672>=</span>device)         <span style=color:#75715e># 还可以f = torch.rand(4, 3, device=torch.device(&#34;cuda&#34;))</span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>    z <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> y
</span></span><span style=display:flex><span>    print(z<span style=color:#f92672>.</span>to(<span style=color:#e6db74>&#34;cpu&#34;</span>, torch<span style=color:#f92672>.</span>int))   <span style=color:#75715e># 还可以再转回cpu中.    z.to(&#34;cpu&#34;) == z.cpu()</span>
</span></span></code></pre></div><hr><h3 id=动量>动量<a hidden class=anchor aria-hidden=true href=#动量>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>package torch<span style=color:#f92672>.</span>optim
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>SGD</span>(Optimizer):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, params, lr<span style=color:#f92672>=</span>required, momentum<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, dampening<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>                 weight_decay<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, nesterov<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 上方构造函数的参数中：</span>
</span></span><span style=display:flex><span>	<span style=color:#75715e># params：模型的参数</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># momentum：动量，即上一次梯度的权重</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># weight_decay：L2正则化的权重</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Adam方法已经自动处理了动量相关</span>
</span></span></code></pre></div><hr><h3 id=学习率衰减>学习率衰减<a hidden class=anchor aria-hidden=true href=#学习率衰减>#</a></h3><p>learning_rate刚开始较大，加快收敛速率。之后逐渐减小，避免震荡。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 第一种方法：监视loss，若发现loss多次不变，则减小learning_rate</span>
</span></span><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>SGD(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span>learning_rate)
</span></span><span style=display:flex><span><span style=color:#75715e># 实例化</span>
</span></span><span style=display:flex><span>scheduler <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>lr_scheduler<span style=color:#f92672>.</span>ReduceLROnPlateau(optimizer<span style=color:#f92672>=</span>optimizer, mode<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;min&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> t <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>500</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;&#39;&#39;forward pass&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>    y_pred <span style=color:#f92672>=</span> model(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;&#39;&#39;compute loss&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> loss_fn(y_pred, y)
</span></span><span style=display:flex><span>    scheduler<span style=color:#f92672>.</span>step(loss)      <span style=color:#75715e># 监视loss，若发现loss多次不变，则减小learning_rate</span>
</span></span><span style=display:flex><span><span style=color:#f92672>-------------------------------------------------------------------------------------------</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 第二种方法：指定一定的epoch次数后减小learning_rate</span>
</span></span><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>SGD(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span>learning_rate)
</span></span><span style=display:flex><span><span style=color:#75715e># 指定每100次epoch，learning_rate = learning_rate * 0.5</span>
</span></span><span style=display:flex><span>scheduler <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>lr_scheduler<span style=color:#f92672>.</span>StepLR(optimizer<span style=color:#f92672>=</span>optimizer, step_size<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>, gamma<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> t <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>500</span>):
</span></span><span style=display:flex><span>    scheduler<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;&#39;&#39;forward pass&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>    y_pred <span style=color:#f92672>=</span> model(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;&#39;&#39;compute loss&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> loss_fn(y_pred, y)
</span></span></code></pre></div><h2 id=卷积神经网络>卷积神经网络<a hidden class=anchor aria-hidden=true href=#卷积神经网络>#</a></h2><h3 id=卷积操作---torchnnconv2d>卷积操作 torch.nn.Conv2d<a hidden class=anchor aria-hidden=true href=#卷积操作---torchnnconv2d>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Conv2d(in_channels, out_channels, kernel_size, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>                dilation<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, groups<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, padding_mode<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;zeros&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># in_channels:输入图片通道数</span>
</span></span><span style=display:flex><span><span style=color:#75715e># out_channels：输出的通道数，即过滤器（核）的个数 ✖ 输入通道数</span>
</span></span></code></pre></div><p>示例</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>layer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, kernel_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>)   <span style=color:#75715e># 模拟图片</span>
</span></span><span style=display:flex><span>out <span style=color:#f92672>=</span> layer<span style=color:#f92672>.</span>forward(x)    <span style=color:#75715e># 不推荐直接使用 forward 函数</span>
</span></span><span style=display:flex><span>out<span style=color:#f92672>.</span>shape          <span style=color:#75715e># torch.Size([1, 3, 26, 26])</span>
</span></span><span style=display:flex><span><span style=color:#f92672>-------------------------------------------------------------------</span>
</span></span><span style=display:flex><span>out <span style=color:#f92672>=</span> layer(x)     <span style=color:#75715e># 推荐用这个。调用 __call__ ，封装了 hooks 和 forward 。作用与 forward 相同。</span>
</span></span><span style=display:flex><span>out<span style=color:#f92672>.</span>shape          <span style=color:#75715e># torch.Size([1, 3, 26, 26])</span>
</span></span></code></pre></div><p>torch.nn.functional.conv2d 函数也可以实现卷积操作，但是不推荐。</p><hr><h3 id=池化操作---torchnnmaxpool2d>池化操作 torch.nn.MaxPool2d<a hidden class=anchor aria-hidden=true href=#池化操作---torchnnmaxpool2d>#</a></h3><p>池化 就是 <strong>降采样</strong>，将原本图像缩小。分为 最大池化 和 平均池化</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>MaxPool2d(kernel_size, stride<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, dilation<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, return_indices<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>				 ceil_mode<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># kernel_size：核窗口大小</span>
</span></span><span style=display:flex><span><span style=color:#75715e># stride： 步长</span>
</span></span></code></pre></div><p>示例</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>pooling <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>MaxPool2d(<span style=color:#ae81ff>2</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)   <span style=color:#75715e># 最大池化</span>
</span></span><span style=display:flex><span>res <span style=color:#f92672>=</span> pooling(out)
</span></span><span style=display:flex><span>res<span style=color:#f92672>.</span>shape        <span style=color:#75715e># torch.Size([1, 3, 13, 13])</span>
</span></span></code></pre></div><p>torch.nn.functional.avg_pool2d 函数也可以实现池化操作，但是不推荐。</p><p><strong>上采样</strong>&ndash;将原本图像放大 torch.nn.functional.interpolate</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>interpolate(input, size<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, scale_factor<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, mode<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;nearest&#39;</span>, 
</span></span><span style=display:flex><span>                                align_corners<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, recompute_scale_factor<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># input (Tensor) – the input tensor   需要放大的图像</span>
</span></span><span style=display:flex><span><span style=color:#75715e># scale_factor ： 放大倍数</span>
</span></span><span style=display:flex><span><span style=color:#75715e># mode ： 放大的方法。nearest 表示用最近的像素点来填充放大后周围的像素点</span>
</span></span></code></pre></div><p><strong>ReLU</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>ReLU(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># inplace： 即 in-place 运算符，将运算结果保存在自身。默认为False。一般设为True，减少内存消耗</span>
</span></span></code></pre></div><hr><h3 id=标准化-batch-norm>标准化 Batch Norm<a hidden class=anchor aria-hidden=true href=#标准化-batch-norm>#</a></h3><p>假设使用 sigmoid 函数作为激活函数，则 当数据量偏大或偏小时会出现梯度消失。故应将数据标准化为标准分布，这样会使数据有好的收敛效果且模型更稳定。（训练和测试要切换）</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>BatchNorm1d(num_features, eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-05</span>, momentum<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>, affine<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, 
</span></span><span style=display:flex><span>                     track_running_stats<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># num_features：图像的通道数</span>
</span></span></code></pre></div><p>示例</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>input <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>784</span>)     <span style=color:#75715e># 28 * 28 == 784</span>
</span></span><span style=display:flex><span>layer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>BatchNorm1d(<span style=color:#ae81ff>16</span>)     <span style=color:#75715e># 也可以不压缩，用二维的方法</span>
</span></span><span style=display:flex><span>out <span style=color:#f92672>=</span> layer(input)
</span></span><span style=display:flex><span>layer<span style=color:#f92672>.</span>running_mean    <span style=color:#75715e># 均值</span>
</span></span><span style=display:flex><span><span style=color:#75715e># tensor([0.0499, 0.0500, 0.0500, 0.0500, 0.0501, 0.0499, 0.0500, 0.0501, 0.0501,</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#         0.0500, 0.0501, 0.0502, 0.0498, 0.0501, 0.0500, 0.0500])</span>
</span></span><span style=display:flex><span>layer<span style=color:#f92672>.</span>running_var     <span style=color:#75715e># 方差</span>
</span></span><span style=display:flex><span><span style=color:#75715e># tensor([0.9083, 0.9083, 0.9083, 0.9084, 0.9083, 0.9084, 0.9083, 0.9083, 0.9083,</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#         0.9083, 0.9084, 0.9083, 0.9083, 0.9083, 0.9084, 0.9083])</span>
</span></span></code></pre></div><p>标准化过程：</p><ul><li>求一个通道内的所有像素的 均值 E[x] 和 方差 Var[x]</li><li>用公式标准化</li><li>之后迭代更新 γ 和 β</li></ul><hr><h3 id=残差神经网络>残差神经网络<a hidden class=anchor aria-hidden=true href=#残差神经网络>#</a></h3><p>普通神经网络在层数达到二十层时，会达到一个瓶颈，即层数继续叠加，效果反而变差。</p><p>残差神经网络在浅层与深层之间增加了一个短路直连，当深层的效果更好时，就选择该值，若更差就退化为浅层神经网络。使得深层网络最差也不会差于浅层神经网络。</p><p>示例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ResBlk</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, in_ch, out_ch):
</span></span><span style=display:flex><span>        super(ResBlk, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>conv1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(in_ch, out_ch, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>bn1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>BatchNorm2d(out_ch)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>relu <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ReLU()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>conv2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(out_ch, out_ch, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>bn2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>BatchNorm2d(out_ch)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>extra <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential()  <span style=color:#75715e># 若输入通道等于输出通道，则直连</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> out_ch <span style=color:#f92672>!=</span> in_ch:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>extra <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(  <span style=color:#75715e># 若输入通道不等于输出通道，则用1 × 1的核函数使其相等</span>
</span></span><span style=display:flex><span>                nn<span style=color:#f92672>.</span>Conv2d(in_ch, out_ch, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>                nn<span style=color:#f92672>.</span>BatchNorm2d(out_ch)
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>bn1(self<span style=color:#f92672>.</span>conv1(x)))
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>bn2(self<span style=color:#f92672>.</span>conv2(out))   <span style=color:#75715e># 完成一个单元你的输出</span>
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>extra(x) <span style=color:#f92672>+</span> out         <span style=color:#75715e># 直连输出与单元输出相加</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> out
</span></span></code></pre></div><hr><p><strong>铺平 torch.nn.Flatten</strong></p><p>卷积神经网络的后边会有一个全连接层，前面的卷积池化操作是二维的，而全连接层是一维的，故中间需要将二维铺平为一维。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Flatten</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        super(Flatten, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, input):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> input<span style=color:#f92672>.</span>view(input<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>0</span>), <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><p><strong>线性容器 torch.nn.Sequential</strong></p><p>按添加的顺序依次执行</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TestNet</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        super(TestNet, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>net <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>16</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>MaxPool2d(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            Flatten(),      <span style=color:#75715e># 即上方定义的Flatten</span>
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>1</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>14</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>14</span>, <span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>net(x)   <span style=color:#75715e># 按线性容器中的顺序执行</span>
</span></span></code></pre></div><p><strong>参数包装 torch.nn.parameter.Parameter</strong></p><p>参数通过该类包装后会自动添加到模型的参数列表中，即<code>model.parameters()</code>。自动求导迭代优化。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MyLinear</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, <span style=color:#f92672>in</span>, out):
</span></span><span style=display:flex><span>        super(MyLinear, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>w <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>parameter<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>randn(out, <span style=color:#f92672>in</span>))
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>b <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>parameter<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>randn(out))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x <span style=color:#f92672>@</span> self<span style=color:#f92672>.</span>w<span style=color:#f92672>.</span>t() <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div><p><strong>数据增强</strong></p><p>通过翻转、旋转、裁剪、缩放等方法，把原来少量的数据集扩大。</p><h2 id=循环神经网络>循环神经网络<a hidden class=anchor aria-hidden=true href=#循环神经网络>#</a></h2><p><strong>GloVe 词向量</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> torchnlp.word_to_vector <span style=color:#f92672>import</span> GloVe
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>vectors <span style=color:#f92672>=</span> GloVe()   <span style=color:#75715e># 全局词向量。词与其向量组成的dict</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>vectors[<span style=color:#e6db74>&#39;hello&#39;</span>]    <span style=color:#75715e># 输出该词的向量</span>
</span></span></code></pre></div><h3 id=torchnnrnn>torch.nn.RNN<a hidden class=anchor aria-hidden=true href=#torchnnrnn>#</a></h3><p><a href=https://blog.csdn.net/SHU15121856/article/details/104387209>torch.nn.RNN参数的详解</a></p><p><a href=https://zhuanlan.zhihu.com/p/32103001>数据如何传入RNN网络</a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 表示feature_len=100, hidden_len=20, 层数=1</span>
</span></span><span style=display:flex><span>rnn <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>RNN(<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 输入3个样本序列(batch=3), 序列长为10(seq_len=10), 每个特征100维度(feature_len=100)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 即有3句话，每句话的单词数为10，每个单词用100维向量表示</span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 传入RNN处理, 另外传入h_0, shape是&lt;层数, batch, hidden_len=20&gt;</span>
</span></span><span style=display:flex><span>out, h <span style=color:#f92672>=</span> rnn(x, torch<span style=color:#f92672>.</span>zeros(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>20</span>))
</span></span><span style=display:flex><span><span style=color:#75715e># 输出返回的out和最终的隐藏记忆单元的shape</span>
</span></span><span style=display:flex><span>print(out<span style=color:#f92672>.</span>shape)  <span style=color:#75715e># torch.Size([10, 3, 20])</span>
</span></span><span style=display:flex><span>print(h<span style=color:#f92672>.</span>shape)  <span style=color:#75715e># torch.Size([1, 3, 20])</span>
</span></span></code></pre></div><p>对于一个RNN单元的输出 h~t ，乘以权重矩阵W变为out，乘以权重矩阵V变为下一个时间节点的输入。但是一般好像不存在W和V，因此两个输入是同一个东西。</p><h3 id=lstm>LSTM<a hidden class=anchor aria-hidden=true href=#lstm>#</a></h3><p>LSTM在原理上通过三道门控制当前单词与过去单词的权重，在pytorch实现上面仅是增加了一个参数 c 来表示过去的记忆。</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://ethereal-lu.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pandas%E5%AD%A6%E4%B9%A0/><span class=title>« Prev</span><br><span>Pandas笔记</span>
</a><a class=next href=https://ethereal-lu.github.io/posts/%E5%B0%8F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%95%B0%E7%BB%84%E8%B5%8B%E5%80%BC/><span class=title>Next »</span><br><span>c语言数组赋值问题</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://ethereal-lu.github.io/>lu</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>