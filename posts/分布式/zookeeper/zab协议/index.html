<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ZAB协议 | lu</title>
<meta name=keywords content><meta name=description content="1、ZAB协议介绍
Zookeeper Atomic Broadcast，是 Zookeeper 专门设计的一种支持崩溃恢复的原子广播协议。用来实现分布式数据一致性。包含两种基本模式，分别是 崩溃恢复、原子广播。
当整个集群在启动时，或当leader 节点崩溃时，ZAB协议就会进入恢复模式并选举产生新的leader，当leader选举出来后，并且集群中有过半机器和该leader节点完成数据同步后，ZAB协议就会退出恢复模式进入消息广播模式。
2、崩溃恢复
崩溃恢复阶段的任务主要为 Leader 选举和数据同步。
1、选举基本原则

选举投票必须在同一轮次中进行
事务ID 大的节点优先成为 Leader
服务器ID大的节点优先成为 Leader

2、相关参数

SID：服务器ID
ZXID：事务ID。在集群范围内全局唯一单调递增，是一个64位的数字，高 32 位是Epoch，低 32 位是单调递增的计数器。每次选举会使 Epoch 加 1，每个写操作会使计数器加1，由于写操作全部由Leader执行，故Leader 节点总是保持着最大的事务 ID，然后根据一致性协议向 Flower 同步数据。由于各 Flower 与 Leader 通信的时差，可能某些 Flower 中保存的数据不是最新的，即各 Flower 的事务 ID 可能不同。事务ID越大说明其保存的数据越新。
Epoch：选举轮次。每个服务器都会维护一个名为logicClock的变量，用于标识当前选举的轮次。每开始一次Leader选举，服务器都会将自己存储的logicClock执行加一操作，并且投票时会附带上这个logicClock。如果其他服务器收到了一个带有旧的logicClock的投票，则会直接忽略这个投票。若一个节点中途宕机之后又连接上，由于其保存的是旧的logicClock，则其不能参与本次选举。logicClock变量只在一次Leader选举开始时执行一次递增操作，一次选举中的多轮投票并不会改变logicClock变量的值。

3、节点状态

LOOKING：正在选举。在选举阶段集群不能对外提供服务。
FOLLOWING：跟随者状态。
LEADING：领导者状态。
OBSERVING：观察者状态。

4、选举流程

自增选举轮次：每开始一次新的选举，节点先将自己的logicClock加 1，并且投票时会附带上这个logicClock。
初始化选票：每个服务器在广播自己的选票前，会将自己的投票箱清空。投票箱记录了所收到的选票。票箱中只会记录每一个投票者的最新的选票。实际上投票箱是一个 Map，节点 id 为键，选票为值，如果收到某个节点的新选票，更新该 Map 即可。
发送选票：节点给自己投票，将投给自己的选票放入投票箱，然后广播该选票，用于与其它节点交换选票信息。
接收其他节点广播的选票：对于接收到的其他节点的选票进行logicClock的校验

如果收到的选票中的logicClock大于自己的logicClock，则清空自己的投票箱，并更新自己的logicClock，然后进行步骤5
如果收到的选票中的logicClock小于自己的logicClock，直接丢弃
如果收到的选票中的logicClock等于自己的logicClock，进行步骤5


选票比较：首先比较 ZXID，如果收到的选票中的 ZXID 较大，则更新自己的选票。如果 ZXID 一致，则比较 SID，如果收到的选票中的 SID较大，则更新自己的选票。将收到的选票存入投票箱。
再次发送选票：进过步骤 5 的选票比较，如果自己的选票需要改变，则修改自己的选票并更新到投票箱，然后再次广播出去。
统计选票：统计自己的投票箱，如果超过半数的节点投票一致，则终止投票。否则继续接收其他节点的投票。
更新节点状态：若过半的票投给了自己，则将自己的服务器状态更新为LEADING，否则将自己的状态更新为FOLLOWING。

6、选举时机
当集群第一次启动时会发起一次选举；当 Leader 崩溃后会发起一次选举。"><meta name=author content="lu"><link rel=canonical href=https://ethereal-lu.github.io/posts/%E5%88%86%E5%B8%83%E5%BC%8F/zookeeper/zab%E5%8D%8F%E8%AE%AE/><link crossorigin=anonymous href=/assets/css/stylesheet.d72444526d7ecbdb0015438a7fa89054a658bf759d0542e2e5df81ce94b493ee.css integrity rel="preload stylesheet" as=style><link rel=icon href=https://ethereal-lu.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ethereal-lu.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ethereal-lu.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://ethereal-lu.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ethereal-lu.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ethereal-lu.github.io/posts/%E5%88%86%E5%B8%83%E5%BC%8F/zookeeper/zab%E5%8D%8F%E8%AE%AE/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://ethereal-lu.github.io/posts/%E5%88%86%E5%B8%83%E5%BC%8F/zookeeper/zab%E5%8D%8F%E8%AE%AE/"><meta property="og:site_name" content="lu"><meta property="og:title" content="ZAB协议"><meta property="og:description" content="1、ZAB协议介绍 Zookeeper Atomic Broadcast，是 Zookeeper 专门设计的一种支持崩溃恢复的原子广播协议。用来实现分布式数据一致性。包含两种基本模式，分别是 崩溃恢复、原子广播。
当整个集群在启动时，或当leader 节点崩溃时，ZAB协议就会进入恢复模式并选举产生新的leader，当leader选举出来后，并且集群中有过半机器和该leader节点完成数据同步后，ZAB协议就会退出恢复模式进入消息广播模式。
2、崩溃恢复 崩溃恢复阶段的任务主要为 Leader 选举和数据同步。
1、选举基本原则 选举投票必须在同一轮次中进行 事务ID 大的节点优先成为 Leader 服务器ID大的节点优先成为 Leader 2、相关参数 SID：服务器ID ZXID：事务ID。在集群范围内全局唯一单调递增，是一个64位的数字，高 32 位是Epoch，低 32 位是单调递增的计数器。每次选举会使 Epoch 加 1，每个写操作会使计数器加1，由于写操作全部由Leader执行，故Leader 节点总是保持着最大的事务 ID，然后根据一致性协议向 Flower 同步数据。由于各 Flower 与 Leader 通信的时差，可能某些 Flower 中保存的数据不是最新的，即各 Flower 的事务 ID 可能不同。事务ID越大说明其保存的数据越新。 Epoch：选举轮次。每个服务器都会维护一个名为logicClock的变量，用于标识当前选举的轮次。每开始一次Leader选举，服务器都会将自己存储的logicClock执行加一操作，并且投票时会附带上这个logicClock。如果其他服务器收到了一个带有旧的logicClock的投票，则会直接忽略这个投票。若一个节点中途宕机之后又连接上，由于其保存的是旧的logicClock，则其不能参与本次选举。logicClock变量只在一次Leader选举开始时执行一次递增操作，一次选举中的多轮投票并不会改变logicClock变量的值。 3、节点状态 LOOKING：正在选举。在选举阶段集群不能对外提供服务。 FOLLOWING：跟随者状态。 LEADING：领导者状态。 OBSERVING：观察者状态。 4、选举流程 自增选举轮次：每开始一次新的选举，节点先将自己的logicClock加 1，并且投票时会附带上这个logicClock。 初始化选票：每个服务器在广播自己的选票前，会将自己的投票箱清空。投票箱记录了所收到的选票。票箱中只会记录每一个投票者的最新的选票。实际上投票箱是一个 Map，节点 id 为键，选票为值，如果收到某个节点的新选票，更新该 Map 即可。 发送选票：节点给自己投票，将投给自己的选票放入投票箱，然后广播该选票，用于与其它节点交换选票信息。 接收其他节点广播的选票：对于接收到的其他节点的选票进行logicClock的校验 如果收到的选票中的logicClock大于自己的logicClock，则清空自己的投票箱，并更新自己的logicClock，然后进行步骤5 如果收到的选票中的logicClock小于自己的logicClock，直接丢弃 如果收到的选票中的logicClock等于自己的logicClock，进行步骤5 选票比较：首先比较 ZXID，如果收到的选票中的 ZXID 较大，则更新自己的选票。如果 ZXID 一致，则比较 SID，如果收到的选票中的 SID较大，则更新自己的选票。将收到的选票存入投票箱。 再次发送选票：进过步骤 5 的选票比较，如果自己的选票需要改变，则修改自己的选票并更新到投票箱，然后再次广播出去。 统计选票：统计自己的投票箱，如果超过半数的节点投票一致，则终止投票。否则继续接收其他节点的投票。 更新节点状态：若过半的票投给了自己，则将自己的服务器状态更新为LEADING，否则将自己的状态更新为FOLLOWING。 6、选举时机 当集群第一次启动时会发起一次选举；当 Leader 崩溃后会发起一次选举。"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-05-08T19:42:15+00:00"><meta property="article:modified_time" content="2022-05-08T19:42:15+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="ZAB协议"><meta name=twitter:description content="1、ZAB协议介绍
Zookeeper Atomic Broadcast，是 Zookeeper 专门设计的一种支持崩溃恢复的原子广播协议。用来实现分布式数据一致性。包含两种基本模式，分别是 崩溃恢复、原子广播。
当整个集群在启动时，或当leader 节点崩溃时，ZAB协议就会进入恢复模式并选举产生新的leader，当leader选举出来后，并且集群中有过半机器和该leader节点完成数据同步后，ZAB协议就会退出恢复模式进入消息广播模式。
2、崩溃恢复
崩溃恢复阶段的任务主要为 Leader 选举和数据同步。
1、选举基本原则

选举投票必须在同一轮次中进行
事务ID 大的节点优先成为 Leader
服务器ID大的节点优先成为 Leader

2、相关参数

SID：服务器ID
ZXID：事务ID。在集群范围内全局唯一单调递增，是一个64位的数字，高 32 位是Epoch，低 32 位是单调递增的计数器。每次选举会使 Epoch 加 1，每个写操作会使计数器加1，由于写操作全部由Leader执行，故Leader 节点总是保持着最大的事务 ID，然后根据一致性协议向 Flower 同步数据。由于各 Flower 与 Leader 通信的时差，可能某些 Flower 中保存的数据不是最新的，即各 Flower 的事务 ID 可能不同。事务ID越大说明其保存的数据越新。
Epoch：选举轮次。每个服务器都会维护一个名为logicClock的变量，用于标识当前选举的轮次。每开始一次Leader选举，服务器都会将自己存储的logicClock执行加一操作，并且投票时会附带上这个logicClock。如果其他服务器收到了一个带有旧的logicClock的投票，则会直接忽略这个投票。若一个节点中途宕机之后又连接上，由于其保存的是旧的logicClock，则其不能参与本次选举。logicClock变量只在一次Leader选举开始时执行一次递增操作，一次选举中的多轮投票并不会改变logicClock变量的值。

3、节点状态

LOOKING：正在选举。在选举阶段集群不能对外提供服务。
FOLLOWING：跟随者状态。
LEADING：领导者状态。
OBSERVING：观察者状态。

4、选举流程

自增选举轮次：每开始一次新的选举，节点先将自己的logicClock加 1，并且投票时会附带上这个logicClock。
初始化选票：每个服务器在广播自己的选票前，会将自己的投票箱清空。投票箱记录了所收到的选票。票箱中只会记录每一个投票者的最新的选票。实际上投票箱是一个 Map，节点 id 为键，选票为值，如果收到某个节点的新选票，更新该 Map 即可。
发送选票：节点给自己投票，将投给自己的选票放入投票箱，然后广播该选票，用于与其它节点交换选票信息。
接收其他节点广播的选票：对于接收到的其他节点的选票进行logicClock的校验

如果收到的选票中的logicClock大于自己的logicClock，则清空自己的投票箱，并更新自己的logicClock，然后进行步骤5
如果收到的选票中的logicClock小于自己的logicClock，直接丢弃
如果收到的选票中的logicClock等于自己的logicClock，进行步骤5


选票比较：首先比较 ZXID，如果收到的选票中的 ZXID 较大，则更新自己的选票。如果 ZXID 一致，则比较 SID，如果收到的选票中的 SID较大，则更新自己的选票。将收到的选票存入投票箱。
再次发送选票：进过步骤 5 的选票比较，如果自己的选票需要改变，则修改自己的选票并更新到投票箱，然后再次广播出去。
统计选票：统计自己的投票箱，如果超过半数的节点投票一致，则终止投票。否则继续接收其他节点的投票。
更新节点状态：若过半的票投给了自己，则将自己的服务器状态更新为LEADING，否则将自己的状态更新为FOLLOWING。

6、选举时机
当集群第一次启动时会发起一次选举；当 Leader 崩溃后会发起一次选举。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://ethereal-lu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"ZAB协议","item":"https://ethereal-lu.github.io/posts/%E5%88%86%E5%B8%83%E5%BC%8F/zookeeper/zab%E5%8D%8F%E8%AE%AE/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ZAB协议","name":"ZAB协议","description":"1、ZAB协议介绍 Zookeeper Atomic Broadcast，是 Zookeeper 专门设计的一种支持崩溃恢复的原子广播协议。用来实现分布式数据一致性。包含两种基本模式，分别是 崩溃恢复、原子广播。\n当整个集群在启动时，或当leader 节点崩溃时，ZAB协议就会进入恢复模式并选举产生新的leader，当leader选举出来后，并且集群中有过半机器和该leader节点完成数据同步后，ZAB协议就会退出恢复模式进入消息广播模式。\n2、崩溃恢复 崩溃恢复阶段的任务主要为 Leader 选举和数据同步。\n1、选举基本原则 选举投票必须在同一轮次中进行 事务ID 大的节点优先成为 Leader 服务器ID大的节点优先成为 Leader 2、相关参数 SID：服务器ID ZXID：事务ID。在集群范围内全局唯一单调递增，是一个64位的数字，高 32 位是Epoch，低 32 位是单调递增的计数器。每次选举会使 Epoch 加 1，每个写操作会使计数器加1，由于写操作全部由Leader执行，故Leader 节点总是保持着最大的事务 ID，然后根据一致性协议向 Flower 同步数据。由于各 Flower 与 Leader 通信的时差，可能某些 Flower 中保存的数据不是最新的，即各 Flower 的事务 ID 可能不同。事务ID越大说明其保存的数据越新。 Epoch：选举轮次。每个服务器都会维护一个名为logicClock的变量，用于标识当前选举的轮次。每开始一次Leader选举，服务器都会将自己存储的logicClock执行加一操作，并且投票时会附带上这个logicClock。如果其他服务器收到了一个带有旧的logicClock的投票，则会直接忽略这个投票。若一个节点中途宕机之后又连接上，由于其保存的是旧的logicClock，则其不能参与本次选举。logicClock变量只在一次Leader选举开始时执行一次递增操作，一次选举中的多轮投票并不会改变logicClock变量的值。 3、节点状态 LOOKING：正在选举。在选举阶段集群不能对外提供服务。 FOLLOWING：跟随者状态。 LEADING：领导者状态。 OBSERVING：观察者状态。 4、选举流程 自增选举轮次：每开始一次新的选举，节点先将自己的logicClock加 1，并且投票时会附带上这个logicClock。 初始化选票：每个服务器在广播自己的选票前，会将自己的投票箱清空。投票箱记录了所收到的选票。票箱中只会记录每一个投票者的最新的选票。实际上投票箱是一个 Map，节点 id 为键，选票为值，如果收到某个节点的新选票，更新该 Map 即可。 发送选票：节点给自己投票，将投给自己的选票放入投票箱，然后广播该选票，用于与其它节点交换选票信息。 接收其他节点广播的选票：对于接收到的其他节点的选票进行logicClock的校验 如果收到的选票中的logicClock大于自己的logicClock，则清空自己的投票箱，并更新自己的logicClock，然后进行步骤5 如果收到的选票中的logicClock小于自己的logicClock，直接丢弃 如果收到的选票中的logicClock等于自己的logicClock，进行步骤5 选票比较：首先比较 ZXID，如果收到的选票中的 ZXID 较大，则更新自己的选票。如果 ZXID 一致，则比较 SID，如果收到的选票中的 SID较大，则更新自己的选票。将收到的选票存入投票箱。 再次发送选票：进过步骤 5 的选票比较，如果自己的选票需要改变，则修改自己的选票并更新到投票箱，然后再次广播出去。 统计选票：统计自己的投票箱，如果超过半数的节点投票一致，则终止投票。否则继续接收其他节点的投票。 更新节点状态：若过半的票投给了自己，则将自己的服务器状态更新为LEADING，否则将自己的状态更新为FOLLOWING。 6、选举时机 当集群第一次启动时会发起一次选举；当 Leader 崩溃后会发起一次选举。\n","keywords":[],"articleBody":"1、ZAB协议介绍 Zookeeper Atomic Broadcast，是 Zookeeper 专门设计的一种支持崩溃恢复的原子广播协议。用来实现分布式数据一致性。包含两种基本模式，分别是 崩溃恢复、原子广播。\n当整个集群在启动时，或当leader 节点崩溃时，ZAB协议就会进入恢复模式并选举产生新的leader，当leader选举出来后，并且集群中有过半机器和该leader节点完成数据同步后，ZAB协议就会退出恢复模式进入消息广播模式。\n2、崩溃恢复 崩溃恢复阶段的任务主要为 Leader 选举和数据同步。\n1、选举基本原则 选举投票必须在同一轮次中进行 事务ID 大的节点优先成为 Leader 服务器ID大的节点优先成为 Leader 2、相关参数 SID：服务器ID ZXID：事务ID。在集群范围内全局唯一单调递增，是一个64位的数字，高 32 位是Epoch，低 32 位是单调递增的计数器。每次选举会使 Epoch 加 1，每个写操作会使计数器加1，由于写操作全部由Leader执行，故Leader 节点总是保持着最大的事务 ID，然后根据一致性协议向 Flower 同步数据。由于各 Flower 与 Leader 通信的时差，可能某些 Flower 中保存的数据不是最新的，即各 Flower 的事务 ID 可能不同。事务ID越大说明其保存的数据越新。 Epoch：选举轮次。每个服务器都会维护一个名为logicClock的变量，用于标识当前选举的轮次。每开始一次Leader选举，服务器都会将自己存储的logicClock执行加一操作，并且投票时会附带上这个logicClock。如果其他服务器收到了一个带有旧的logicClock的投票，则会直接忽略这个投票。若一个节点中途宕机之后又连接上，由于其保存的是旧的logicClock，则其不能参与本次选举。logicClock变量只在一次Leader选举开始时执行一次递增操作，一次选举中的多轮投票并不会改变logicClock变量的值。 3、节点状态 LOOKING：正在选举。在选举阶段集群不能对外提供服务。 FOLLOWING：跟随者状态。 LEADING：领导者状态。 OBSERVING：观察者状态。 4、选举流程 自增选举轮次：每开始一次新的选举，节点先将自己的logicClock加 1，并且投票时会附带上这个logicClock。 初始化选票：每个服务器在广播自己的选票前，会将自己的投票箱清空。投票箱记录了所收到的选票。票箱中只会记录每一个投票者的最新的选票。实际上投票箱是一个 Map，节点 id 为键，选票为值，如果收到某个节点的新选票，更新该 Map 即可。 发送选票：节点给自己投票，将投给自己的选票放入投票箱，然后广播该选票，用于与其它节点交换选票信息。 接收其他节点广播的选票：对于接收到的其他节点的选票进行logicClock的校验 如果收到的选票中的logicClock大于自己的logicClock，则清空自己的投票箱，并更新自己的logicClock，然后进行步骤5 如果收到的选票中的logicClock小于自己的logicClock，直接丢弃 如果收到的选票中的logicClock等于自己的logicClock，进行步骤5 选票比较：首先比较 ZXID，如果收到的选票中的 ZXID 较大，则更新自己的选票。如果 ZXID 一致，则比较 SID，如果收到的选票中的 SID较大，则更新自己的选票。将收到的选票存入投票箱。 再次发送选票：进过步骤 5 的选票比较，如果自己的选票需要改变，则修改自己的选票并更新到投票箱，然后再次广播出去。 统计选票：统计自己的投票箱，如果超过半数的节点投票一致，则终止投票。否则继续接收其他节点的投票。 更新节点状态：若过半的票投给了自己，则将自己的服务器状态更新为LEADING，否则将自己的状态更新为FOLLOWING。 6、选举时机 当集群第一次启动时会发起一次选举；当 Leader 崩溃后会发起一次选举。\n6.1、第一次启动 假设Zookeeper 集群中有 5 个ZooKeeper节点，且是第一次启动，都不包含数据。节点按从 1 到 5 的顺序依次启动，则选举流程如下：\n① 启动，发起一次选举。① 投自己一票。此时 ① 票数为1，不够半数以上（5÷2=2.5，3票），选举无法完成，① 状态保持为LOOKING； ② 启动，再发起一次选举。① 和 ② 分别投自己一票并交换选票信息：此时 ① 发现 ② 的myid比自己的myid大，把自己的票数投给了 ②。此时 ① 票数为0票， ② 票数为2票，没有半数以上结果，选举无法完成，①、 ② 状态保持为LOOKING ③ 启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：① 为0票，② 为0票，服务器3为3票。此时 ③ 的票数已经超过半数，服务器3当选Leader。①，② 状态更改为FOLLOWING，③ 更改状态为LEADING； ④ 启动，发起一次选举。此时①，②，③已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：③ 为3票，④ 为1票。此时 ④ 服从多数，更改选票信息为 ③，并更改状态为FOLLOWING； ⑤ 启动，同 ④ 一样，最终 ③ 为leader，其他为follower。 6.2、 Leader 崩溃 Leader 选举成功后，Leader 会周期性地向 follower 发送心跳（ping命令，没有内容的 socket）。当 Leader 崩溃后，Follower 发现通道关闭，则 Follower 的状态变为 LOOKING，开始进行选举。\n7、数据同步 在选举结束后还需要进行数据同步。先同步 epoch，然后 Follower 将自己的 ZXID 发送给 Leader，当收到半数以上的 Follower 发送的 ZXID 后，Leader 根据 ZXID 对每个 Follower 给出不同的同步策略。\nDIFF：如果 Follower 的记录和 Leader 的记录相差的不多，使用增量同步的方式将一个一个写请求发送给 Follower SNAP：如果 Follower 的记录和当前 Leader 相差太多，Leader 直接将自己的整个内存数据发送给 Follower TRUNC：如果 Follower 的记录比 Leader 的记录更全，Follower 自行把多余的部分给截断，降级到和 Leader 一致 TRUNC+DIFF：先把多余的部分截断，再进行增量同步 7.1、DIFF（直接差异化同步） 每个 ZK 节点将写请求存储在写请求队列，如果队列达到上限，将 ZXID 最小的写请求移除。\n如果 Follower 上报的 ZXID 在写请求队列的 minZXID 和 maxZXID 之间，就将 ZXID 之后的写请求一条一条地发送给 Follower 。\n7.2、SNAP（全量同步） 如果 Follower 上报的 ZXID 在小于 minZXID，Leader 将内存中的数据全部序列化并发送给 Follower 。Follower 会将自己的数据全部清空并将接收到的数据反序列化存储。\n7.3、TRUNC（仅回滚同步） 如果 Follower 上报的 ZXID 在大于 maxZXID，Follower 自行把多余的数据截断。\n出现这种情况可能是 Leader 掉线了一段时间，在此期间各 Follower 已经重新选举了新的Leader，当旧Leader上线后自动成为Follower，但是新的 Leader 的 ZXID 小于自己。\n7.4、TRUNC+DIFF（先回滚再差异化同步） 这是一种罕见的情况。假如有A、B、C三台机器，B是Leader，此时的Leader_epoch为5，当前已经被集群中大部分机器都提交的ZXID包括：0x500000001和0x500000002。此时，Leader正要处理0x500000003，并且已经将该事务写入到了Leader本地的事务日志中，就在Leader要将该Proposal发送给其他Follower机器进行投票的时候，Leader服务器挂了，开始新一轮选举，同时Leader_epoch变更为6，之后A和C继续对外进行服务，又提交了0x600000001和0x600000002两个事务。此时B再次启动，并开始数据同步，那么 B 需要先回滚到 0x500000002，再差异化同步到0x600000002。\n当数据同步之后，Zookeeper 集群才可以对外开放服务。根据 CAP 理论，如果在数据同步阶段允许对外开放服务，就是 AP 模型；否则是 CP 模型，Zookeeper 显然是 CP 模型。\n数据同步详见此处\n3、消息广播 在 Zookeeper 集群中，任何一个写事务都是分布式事务。在消息广播阶段 Zookeeper 通过类似于 2PC 的方式处理主从数据同步。\n客户端会随机连接 Zookeeper 集群中的节点 ，如果集群中的follewer接收到来自客户端的写请求，follower会将请求转发给 leader，统一交给leader来处理。\n主从数据同步步骤如下：\n客户端向主节点发起事务请求 主节点将事务写到本地事务日志，并给自己返回一个 ACK，同时将数据发送给从节点 从节点将事务写到本地事务日志，并给主节点返回一个 ACK 主节点统计收到的 ACK 数，加上自己的 ACK，如果收到的 ACK 数量超过集群总数的一半，则向从节点发送 commit，同时本地也提交事务。 各节点接收到 commit 后提交事务，将事务操作应用到内存中。 之所以说该过程类似于 2PC，是因为 2PC 要求所有的节点全部执行成功才提交事务，而 ZAB 只需要超过半数节点执行成功就可以提交\nObserver 不参与投票过程，不会返回 ack，但其必须要与 Leader 保持数据一致性。\n4、数据一致性 根据上方数据同步过程可知，Leader 总是与超过半数的从节点保持强一致性，但是与其他的从节点并没有保持强一致性。\nZookeeper 在数据同步时追求的并不是强一致性，而是顺序一致性。\n1、顺序一致性 Zookeeper 的所有写操作都通过主节点进行，这样所有节点的事务执行顺序都和主节点相同，不会出现某个节点的事务执行顺序与其它节点不同的情况。\nZookeeper 是通过 ZXID 来保证顺序一致性的，由于在同一任期内 zxid 是连续递增的，每个节点都会通过比较自身的最新 zxid 和 当前提案的 zxid 是否差1来保证事务严格按照顺序生效。Leader 会将所有未提交的事务存储在 ConcurrentHashMap 中，键为 zxid，值为提案的信息，当事务提交后就删除这个键值对。每次 Leader 要提交事务时都先查看 ConcurrentHashMap 是否存在 zxid - 1 的键，若存在说明在它之前还有事务没有提交，则当前事务就不能提交，如此保证事务执行的顺序一致性。\n但是Zookeeper 允许客户端从从节点读取数据，因此如果客户端在读取过程中连接了不同的节点，则顺序一致性就得不到保证了，即客户端先在一个节点读取了新的数据，然后到另一个节点读取到了旧数据。为了解决这种情况，Zookeeper 保证了单一视图。\n2、单一视图 对于客户端的每次请求，服务器都会返回自己当前最新的事务id。当客户端与服务器断开之后又重新连接时，会将自己记录的事务id 发送给服务端，服务端如果发现收到的事务 id 在本地不存在，就会拒绝连接请求，之后客户端再尝试连接其他服务器。从而保证客户端不会连接到数据版本比自己之前看到的旧的服务器上，保证了单一视图特性。\n5、Zookeeper的脑裂问题 脑裂问题是指由于网络等原因导致集群被分割为俩个团体，团体内部可以通信，团体之间不可以通信。这时必然有一个团体内是没有 Leader 的，于是该团体就会选举一个 Leader。这样，原本作为一个整体的集群出现了两个 Leader，这就脑裂现象。\nZookeeper 由于实行半数原则，如果出现两个团体，则要么这两个团体的成员数量都不过半，此时两个团体都选举 Leader 失败，即整个集群不能提供服务；要么一个团体成员数量过半，另一个不过半，此时过半的团体正常提供服务，而不过半的团体则无法选举 leader 不能提供服务。\n因此 Zookeeper 不存在脑裂问题。\n假设: leader发生了假死, followers选举出了一个新的leader.当旧的leader复活并认为自己仍然是leader, 它向其他followers发出写请求时, 会被拒绝.—— 因为ZooKeeper维护了一个叫epoch的变量, 每当新leader产生时, epoch都会递增, followers如果确认了新的leader存在, 同时也会知道其epoch的值 —— 它们会拒绝epoch小于现任leader的epoch的所有旧leader的任何请求.\n6、ZAB与Raft区别 ZAB会在每次leader选举之后进行一次数据同步，所以在一个轮次内，节点间的日志内容是统一的。而Raft在leader选举之后没有同步过程，因此可能出现已经隔了几个轮次，节点间的日志还不一致。 在一次选举leader的过程中，Raft只投一次票，每个竞选者会随机睡一段时间，睡醒就发送选票信息请求他人给自己投票，因此越早睡醒的节点成为leader的可能性越大；ZAB可能会在一次选举的过程中发送多次投票结果，这种方式有利于选举出zxid更大的节点，但是选举时间理论上比Raft要多。 在消息广播阶段，ZK收到写请求后将事务写到事务日志中，同时广播给从节点，当收到一半以上ack后广播从节点提交事物。Raft同样将写请求转发到主节点，主节点写入事务日志，同时可能将一批事务广播给从节点，收到一半以上ack时，只有主节点提交事务，不向从节点广播提交的消息，当下一次心跳时才顺带告知从节点自己已经提交，然后从节点提交自己的事务。 ","wordCount":"361","inLanguage":"en","datePublished":"2022-05-08T19:42:15Z","dateModified":"2022-05-08T19:42:15Z","author":{"@type":"Person","name":"lu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ethereal-lu.github.io/posts/%E5%88%86%E5%B8%83%E5%BC%8F/zookeeper/zab%E5%8D%8F%E8%AE%AE/"},"publisher":{"@type":"Organization","name":"lu","logo":{"@type":"ImageObject","url":"https://ethereal-lu.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ethereal-lu.github.io/ accesskey=h title="lu (Alt + H)">lu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://ethereal-lu.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://ethereal-lu.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ethereal-lu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://ethereal-lu.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">ZAB协议</h1><div class=post-meta><span title='2022-05-08 19:42:15 +0000 UTC'>2022-05-08</span>&nbsp;·&nbsp;361 words&nbsp;·&nbsp;lu</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1选举基本原则>1、选举基本原则</a></li><li><a href=#2相关参数>2、相关参数</a></li><li><a href=#3节点状态>3、节点状态</a></li><li><a href=#4选举流程>4、选举流程</a></li><li><a href=#6选举时机>6、选举时机</a><ul><li><a href=#61第一次启动>6.1、第一次启动</a></li><li><a href=#62-leader-崩溃>6.2、 Leader 崩溃</a></li></ul></li><li><a href=#7数据同步>7、数据同步</a><ul><li><a href=#71diff直接差异化同步>7.1、DIFF（直接差异化同步）</a></li><li><a href=#72snap全量同步>7.2、SNAP（全量同步）</a></li><li><a href=#73trunc仅回滚同步>7.3、TRUNC（仅回滚同步）</a></li><li><a href=#74truncdiff先回滚再差异化同步>7.4、TRUNC+DIFF（先回滚再差异化同步）</a></li></ul></li></ul><ul><li><a href=#1顺序一致性>1、顺序一致性</a></li><li><a href=#2单一视图>2、单一视图</a></li></ul></nav></div></details></div><div class=post-content><h1 id=1zab协议介绍>1、ZAB协议介绍<a hidden class=anchor aria-hidden=true href=#1zab协议介绍>#</a></h1><p>Zookeeper Atomic Broadcast，是 Zookeeper 专门设计的一种支持崩溃恢复的原子广播协议。用来实现分布式数据一致性。包含两种基本模式，分别是 崩溃恢复、原子广播。</p><p>当整个集群在启动时，或当leader 节点崩溃时，ZAB协议就会进入恢复模式并选举产生新的leader，当leader选举出来后，并且集群中有过半机器和该leader节点完成数据同步后，ZAB协议就会退出恢复模式进入消息广播模式。</p><h1 id=2崩溃恢复>2、崩溃恢复<a hidden class=anchor aria-hidden=true href=#2崩溃恢复>#</a></h1><p>崩溃恢复阶段的任务主要为 Leader 选举和数据同步。</p><h2 id=1选举基本原则>1、选举基本原则<a hidden class=anchor aria-hidden=true href=#1选举基本原则>#</a></h2><ol><li>选举投票必须在同一轮次中进行</li><li>事务ID 大的节点优先成为 Leader</li><li>服务器ID大的节点优先成为 Leader</li></ol><h2 id=2相关参数>2、相关参数<a hidden class=anchor aria-hidden=true href=#2相关参数>#</a></h2><ul><li>SID：服务器ID</li><li>ZXID：事务ID。在集群范围内全局唯一单调递增，是一个64位的数字，高 32 位是Epoch，低 32 位是单调递增的计数器。每次选举会使 Epoch 加 1，每个写操作会使计数器加1，由于写操作全部由Leader执行，故Leader 节点总是保持着最大的事务 ID，然后根据一致性协议向 Flower 同步数据。由于各 Flower 与 Leader 通信的时差，可能某些 Flower 中保存的数据不是最新的，即各 Flower 的事务 ID 可能不同。事务ID越大说明其保存的数据越新。</li><li>Epoch：选举轮次。每个服务器都会维护一个名为<code>logicClock</code>的变量，用于标识当前选举的轮次。每开始一次Leader选举，服务器都会将自己存储的<code>logicClock</code>执行加一操作，并且投票时会附带上这个<code>logicClock</code>。如果其他服务器收到了一个带有旧的<code>logicClock</code>的投票，则会直接忽略这个投票。若一个节点中途宕机之后又连接上，由于其保存的是旧的<code>logicClock</code>，则其不能参与本次选举。<strong><code>logicClock</code>变量只在一次Leader选举开始时执行一次递增操作，一次选举中的多轮投票并不会改变<code>logicClock</code>变量的值。</strong></li></ul><h2 id=3节点状态>3、节点状态<a hidden class=anchor aria-hidden=true href=#3节点状态>#</a></h2><ul><li>LOOKING：正在选举。在选举阶段集群不能对外提供服务。</li><li>FOLLOWING：跟随者状态。</li><li>LEADING：领导者状态。</li><li>OBSERVING：观察者状态。</li></ul><h2 id=4选举流程>4、选举流程<a hidden class=anchor aria-hidden=true href=#4选举流程>#</a></h2><ol><li>自增选举轮次：每开始一次新的选举，节点先将自己的<code>logicClock</code>加 1，并且投票时会附带上这个<code>logicClock</code>。</li><li>初始化选票：每个服务器在广播自己的选票前，会将自己的投票箱清空。投票箱记录了所收到的选票。票箱中只会记录每一个投票者的最新的选票。实际上投票箱是一个 Map，节点 id 为键，选票为值，如果收到某个节点的新选票，更新该 Map 即可。</li><li>发送选票：节点给自己投票，将投给自己的选票放入投票箱，然后广播该选票，用于与其它节点交换选票信息。</li><li>接收其他节点广播的选票：对于接收到的其他节点的选票进行<code>logicClock</code>的校验<ul><li>如果收到的选票中的<code>logicClock</code>大于自己的<code>logicClock</code>，则清空自己的投票箱，并更新自己的<code>logicClock</code>，然后进行步骤5</li><li>如果收到的选票中的<code>logicClock</code>小于自己的<code>logicClock</code>，直接丢弃</li><li>如果收到的选票中的<code>logicClock</code>等于自己的<code>logicClock</code>，进行步骤5</li></ul></li><li>选票比较：首先比较 ZXID，如果收到的选票中的 ZXID 较大，则更新自己的选票。如果 ZXID 一致，则比较 SID，如果收到的选票中的 SID较大，则更新自己的选票。将收到的选票存入投票箱。</li><li>再次发送选票：进过步骤 5 的选票比较，如果自己的选票需要改变，则修改自己的选票并更新到投票箱，然后再次广播出去。</li><li>统计选票：统计自己的投票箱，如果超过半数的节点投票一致，则终止投票。否则继续接收其他节点的投票。</li><li>更新节点状态：若过半的票投给了自己，则将自己的服务器状态更新为LEADING，否则将自己的状态更新为FOLLOWING。</li></ol><h2 id=6选举时机>6、选举时机<a hidden class=anchor aria-hidden=true href=#6选举时机>#</a></h2><p>当集群第一次启动时会发起一次选举；当 Leader 崩溃后会发起一次选举。</p><h3 id=61第一次启动>6.1、第一次启动<a hidden class=anchor aria-hidden=true href=#61第一次启动>#</a></h3><p>假设Zookeeper 集群中有 5 个ZooKeeper节点，且是第一次启动，都不包含数据。节点按从 1 到 5 的顺序依次启动，则选举流程如下：</p><ol><li>① 启动，发起一次选举。① 投自己一票。此时 ① 票数为1，不够半数以上（5÷2=2.5，3票），选举无法完成，① 状态保持为<code>LOOKING</code>；</li><li>② 启动，再发起一次选举。① 和 ② 分别投自己一票并交换选票信息：此时 ① 发现 ② 的<code>myid</code>比自己的<code>myid</code>大，把自己的票数投给了 ②。此时 ① 票数为0票， ② 票数为2票，没有半数以上结果，选举无法完成，①、 ② 状态保持为<code>LOOKING</code></li><li>③ 启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：① 为0票，② 为0票，服务器3为3票。此时 ③ 的票数已经超过半数，服务器3当选Leader。①，② 状态更改为FOLLOWING，③ 更改状态为<code>LEADING</code>；</li><li>④ 启动，发起一次选举。此时①，②，③已经不是<code>LOOKING</code>状态，不会更改选票信息。交换选票信息结果：③ 为3票，④ 为1票。此时 ④ 服从多数，更改选票信息为 ③，并更改状态为<code>FOLLOWING</code>；</li><li>⑤ 启动，同 ④ 一样，最终 ③ 为<code>leader</code>，其他为<code>follower</code>。</li></ol><h3 id=62-leader-崩溃>6.2、 Leader 崩溃<a hidden class=anchor aria-hidden=true href=#62-leader-崩溃>#</a></h3><p>Leader 选举成功后，Leader 会周期性地向 follower 发送心跳（ping命令，没有内容的 socket）。当 Leader 崩溃后，Follower 发现通道关闭，则 Follower 的状态变为 LOOKING，开始进行选举。</p><p><img alt=非第一次启动选举 loading=lazy src=/posts/%E5%88%86%E5%B8%83%E5%BC%8F/zookeeper/zab%E5%8D%8F%E8%AE%AE/%E9%9D%9E%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%90%AF%E5%8A%A8%E9%80%89%E4%B8%BE.png></p><h2 id=7数据同步>7、数据同步<a hidden class=anchor aria-hidden=true href=#7数据同步>#</a></h2><p>在选举结束后还需要进行数据同步。先同步 epoch，然后 Follower 将自己的 ZXID 发送给 Leader，当收到半数以上的 Follower 发送的 ZXID 后，Leader 根据 ZXID 对每个 Follower 给出不同的同步策略。</p><ul><li>DIFF：如果 Follower 的记录和 Leader 的记录相差的不多，使用增量同步的方式将一个一个写请求发送给 Follower</li><li>SNAP：如果 Follower 的记录和当前 Leader 相差太多，Leader 直接将自己的整个内存数据发送给 Follower</li><li>TRUNC：如果 Follower 的记录比 Leader 的记录更全，Follower 自行把多余的部分给截断，降级到和 Leader 一致</li><li>TRUNC+DIFF：先把多余的部分截断，再进行增量同步</li></ul><h3 id=71diff直接差异化同步>7.1、DIFF（直接差异化同步）<a hidden class=anchor aria-hidden=true href=#71diff直接差异化同步>#</a></h3><p>每个 ZK 节点将写请求存储在写请求队列，如果队列达到上限，将 ZXID 最小的写请求移除。</p><p>如果 Follower 上报的 ZXID 在写请求队列的 minZXID 和 maxZXID 之间，就将 ZXID 之后的写请求一条一条地发送给 Follower 。</p><h3 id=72snap全量同步>7.2、SNAP（全量同步）<a hidden class=anchor aria-hidden=true href=#72snap全量同步>#</a></h3><p>如果 Follower 上报的 ZXID 在小于 minZXID，Leader 将内存中的数据全部序列化并发送给 Follower 。Follower 会将自己的数据全部清空并将接收到的数据反序列化存储。</p><h3 id=73trunc仅回滚同步>7.3、TRUNC（仅回滚同步）<a hidden class=anchor aria-hidden=true href=#73trunc仅回滚同步>#</a></h3><p>如果 Follower 上报的 ZXID 在大于 maxZXID，Follower 自行把多余的数据截断。</p><p>出现这种情况可能是 Leader 掉线了一段时间，在此期间各 Follower 已经重新选举了新的Leader，当旧Leader上线后自动成为Follower，但是新的 Leader 的 ZXID 小于自己。</p><h3 id=74truncdiff先回滚再差异化同步>7.4、TRUNC+DIFF（先回滚再差异化同步）<a hidden class=anchor aria-hidden=true href=#74truncdiff先回滚再差异化同步>#</a></h3><p>这是一种罕见的情况。假如有A、B、C三台机器，B是Leader，此时的Leader_epoch为5，当前已经被集群中大部分机器都提交的ZXID包括：0x500000001和0x500000002。此时，Leader正要处理0x500000003，并且已经将该事务写入到了Leader本地的事务日志中，就在Leader要将该Proposal发送给其他Follower机器进行投票的时候，Leader服务器挂了，开始新一轮选举，同时Leader_epoch变更为6，之后A和C继续对外进行服务，又提交了0x600000001和0x600000002两个事务。此时B再次启动，并开始数据同步，那么 B 需要先回滚到 0x500000002，再差异化同步到0x600000002。</p><p>当数据同步之后，Zookeeper 集群才可以对外开放服务。根据 CAP 理论，如果在数据同步阶段允许对外开放服务，就是 AP 模型；否则是 CP 模型，Zookeeper 显然是 CP 模型。</p><p><a href=https://www.cnblogs.com/xueweihan/p/14575954.html>数据同步详见此处</a></p><h1 id=3消息广播>3、消息广播<a hidden class=anchor aria-hidden=true href=#3消息广播>#</a></h1><p>在 Zookeeper 集群中，任何一个写事务都是分布式事务。在消息广播阶段 Zookeeper 通过类似于 2PC 的方式处理<strong>主从数据同步</strong>。</p><p>客户端会随机连接 Zookeeper 集群中的节点 ，如果集群中的follewer接收到来自客户端的写请求，follower会将请求转发给 leader，统一交给leader来处理。</p><p>主从数据同步步骤如下：</p><ol><li>客户端向主节点发起事务请求</li><li>主节点将事务写到本地事务日志，并给自己返回一个 ACK，同时将数据发送给从节点</li><li>从节点将事务写到本地事务日志，并给主节点返回一个 ACK</li><li>主节点统计收到的 ACK 数，加上自己的 ACK，如果收到的 ACK 数量超过集群总数的一半，则向从节点发送 commit，同时本地也提交事务。</li><li>各节点接收到 commit 后提交事务，将事务操作应用到内存中。</li></ol><p>之所以说该过程类似于 2PC，是因为 2PC 要求所有的节点全部执行成功才提交事务，而 ZAB 只需要超过半数节点执行成功就可以提交</p><p>Observer 不参与投票过程，不会返回 ack，但其必须要与 Leader 保持数据一致性。</p><h1 id=4数据一致性>4、数据一致性<a hidden class=anchor aria-hidden=true href=#4数据一致性>#</a></h1><p>根据上方数据同步过程可知，Leader 总是与超过半数的从节点保持强一致性，但是与其他的从节点并没有保持强一致性。</p><p>Zookeeper 在数据同步时追求的并不是强一致性，而是<strong>顺序一致性</strong>。</p><h2 id=1顺序一致性>1、顺序一致性<a hidden class=anchor aria-hidden=true href=#1顺序一致性>#</a></h2><p>Zookeeper 的所有写操作都通过主节点进行，这样所有节点的事务执行顺序都和主节点相同，不会出现某个节点的事务执行顺序与其它节点不同的情况。</p><p>Zookeeper 是通过 ZXID 来保证顺序一致性的，由于在同一任期内 zxid 是连续递增的，每个节点都会通过比较自身的最新 zxid 和 当前提案的 zxid 是否差1来保证事务严格按照顺序生效。Leader 会将所有未提交的事务存储在 ConcurrentHashMap 中，键为 zxid，值为提案的信息，当事务提交后就删除这个键值对。每次 Leader 要提交事务时都先查看 ConcurrentHashMap 是否存在 zxid - 1 的键，若存在说明在它之前还有事务没有提交，则当前事务就不能提交，如此保证事务执行的顺序一致性。</p><p>但是Zookeeper 允许客户端从从节点读取数据，因此如果客户端在读取过程中连接了不同的节点，则顺序一致性就得不到保证了，即客户端先在一个节点读取了新的数据，然后到另一个节点读取到了旧数据。为了解决这种情况，Zookeeper 保证了单一视图。</p><h2 id=2单一视图>2、单一视图<a hidden class=anchor aria-hidden=true href=#2单一视图>#</a></h2><p>对于客户端的每次请求，服务器都会返回自己当前最新的事务id。当客户端与服务器断开之后又重新连接时，会将自己记录的事务id 发送给服务端，服务端如果发现收到的事务 id 在本地不存在，就会拒绝连接请求，之后客户端再尝试连接其他服务器。从而保证客户端不会连接到数据版本比自己之前看到的旧的服务器上，保证了单一视图特性。</p><h1 id=5zookeeper的脑裂问题>5、Zookeeper的脑裂问题<a hidden class=anchor aria-hidden=true href=#5zookeeper的脑裂问题>#</a></h1><p>脑裂问题是指由于网络等原因导致集群被分割为俩个团体，团体内部可以通信，团体之间不可以通信。这时必然有一个团体内是没有 Leader 的，于是该团体就会选举一个 Leader。这样，原本作为一个整体的集群出现了两个 Leader，这就脑裂现象。</p><p>Zookeeper 由于实行半数原则，如果出现两个团体，则要么这两个团体的成员数量都不过半，此时两个团体都选举 Leader 失败，即整个集群不能提供服务；要么一个团体成员数量过半，另一个不过半，此时过半的团体正常提供服务，而不过半的团体则无法选举 leader 不能提供服务。</p><p>因此 Zookeeper 不存在脑裂问题。</p><p>假设: leader发生了假死, followers选举出了一个新的leader.当旧的leader复活并认为自己仍然是leader, 它向其他followers发出写请求时, 会被拒绝.—— 因为ZooKeeper维护了一个叫epoch的变量, 每当新leader产生时, epoch都会递增, followers如果确认了新的leader存在, 同时也会知道其epoch的值 —— 它们会拒绝epoch小于现任leader的epoch的所有旧leader的任何请求.</p><h1 id=6zab与raft区别>6、ZAB与Raft区别<a hidden class=anchor aria-hidden=true href=#6zab与raft区别>#</a></h1><ul><li>ZAB会在每次leader选举之后进行一次数据同步，所以在一个轮次内，节点间的日志内容是统一的。而Raft在leader选举之后没有同步过程，因此可能出现已经隔了几个轮次，节点间的日志还不一致。</li><li>在一次选举leader的过程中，Raft只投一次票，每个竞选者会随机睡一段时间，睡醒就发送选票信息请求他人给自己投票，因此越早睡醒的节点成为leader的可能性越大；ZAB可能会在一次选举的过程中发送多次投票结果，这种方式有利于选举出zxid更大的节点，但是选举时间理论上比Raft要多。</li><li>在消息广播阶段，ZK收到写请求后将事务写到事务日志中，同时广播给从节点，当收到一半以上ack后广播从节点提交事物。Raft同样将写请求转发到主节点，主节点写入事务日志，同时可能将一批事务广播给从节点，收到一半以上ack时，只有主节点提交事务，不向从节点广播提交的消息，当下一次心跳时才顺带告知从节点自己已经提交，然后从节点提交自己的事务。</li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://ethereal-lu.github.io/posts/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/mysql%E9%BB%98%E8%AE%A4%E6%95%B0%E6%8D%AE%E5%BA%93/><span class=title>« Prev</span><br><span>MySQL默认数据库</span>
</a><a class=next href=https://ethereal-lu.github.io/posts/%E5%88%86%E5%B8%83%E5%BC%8F/kafka/kafka%E5%9F%BA%E7%A1%80/><span class=title>Next »</span><br><span>Kafka基础</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://ethereal-lu.github.io/>lu</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>